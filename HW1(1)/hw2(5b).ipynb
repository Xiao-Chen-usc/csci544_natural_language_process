{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ece7a63",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ac65838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c415e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 16148: expected 15 fields, saw 22\\nSkipping line 20100: expected 15 fields, saw 22\\nSkipping line 45178: expected 15 fields, saw 22\\nSkipping line 48700: expected 15 fields, saw 22\\nSkipping line 63331: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 86053: expected 15 fields, saw 22\\nSkipping line 88858: expected 15 fields, saw 22\\nSkipping line 115017: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 137366: expected 15 fields, saw 22\\nSkipping line 139110: expected 15 fields, saw 22\\nSkipping line 165540: expected 15 fields, saw 22\\nSkipping line 171813: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 203723: expected 15 fields, saw 22\\nSkipping line 209366: expected 15 fields, saw 22\\nSkipping line 211310: expected 15 fields, saw 22\\nSkipping line 246351: expected 15 fields, saw 22\\nSkipping line 252364: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 267003: expected 15 fields, saw 22\\nSkipping line 268957: expected 15 fields, saw 22\\nSkipping line 303336: expected 15 fields, saw 22\\nSkipping line 306021: expected 15 fields, saw 22\\nSkipping line 311569: expected 15 fields, saw 22\\nSkipping line 316767: expected 15 fields, saw 22\\nSkipping line 324009: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 359107: expected 15 fields, saw 22\\nSkipping line 368367: expected 15 fields, saw 22\\nSkipping line 381180: expected 15 fields, saw 22\\nSkipping line 390453: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 412243: expected 15 fields, saw 22\\nSkipping line 419342: expected 15 fields, saw 22\\nSkipping line 457388: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 459935: expected 15 fields, saw 22\\nSkipping line 460167: expected 15 fields, saw 22\\nSkipping line 466460: expected 15 fields, saw 22\\nSkipping line 500314: expected 15 fields, saw 22\\nSkipping line 500339: expected 15 fields, saw 22\\nSkipping line 505396: expected 15 fields, saw 22\\nSkipping line 507760: expected 15 fields, saw 22\\nSkipping line 513626: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 527638: expected 15 fields, saw 22\\nSkipping line 534209: expected 15 fields, saw 22\\nSkipping line 535687: expected 15 fields, saw 22\\nSkipping line 547671: expected 15 fields, saw 22\\nSkipping line 549054: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 599929: expected 15 fields, saw 22\\nSkipping line 604776: expected 15 fields, saw 22\\nSkipping line 609937: expected 15 fields, saw 22\\nSkipping line 632059: expected 15 fields, saw 22\\nSkipping line 638546: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 665017: expected 15 fields, saw 22\\nSkipping line 677680: expected 15 fields, saw 22\\nSkipping line 684370: expected 15 fields, saw 22\\nSkipping line 720217: expected 15 fields, saw 29\\n'\n",
      "b'Skipping line 723240: expected 15 fields, saw 22\\nSkipping line 723433: expected 15 fields, saw 22\\nSkipping line 763891: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 800288: expected 15 fields, saw 22\\nSkipping line 802942: expected 15 fields, saw 22\\nSkipping line 803379: expected 15 fields, saw 22\\nSkipping line 805122: expected 15 fields, saw 22\\nSkipping line 821899: expected 15 fields, saw 22\\nSkipping line 831707: expected 15 fields, saw 22\\nSkipping line 842829: expected 15 fields, saw 22\\nSkipping line 843604: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 863904: expected 15 fields, saw 22\\nSkipping line 875655: expected 15 fields, saw 22\\nSkipping line 886796: expected 15 fields, saw 22\\nSkipping line 892299: expected 15 fields, saw 22\\nSkipping line 902518: expected 15 fields, saw 22\\nSkipping line 903079: expected 15 fields, saw 22\\nSkipping line 912678: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 932953: expected 15 fields, saw 22\\nSkipping line 936838: expected 15 fields, saw 22\\nSkipping line 937177: expected 15 fields, saw 22\\nSkipping line 947695: expected 15 fields, saw 22\\nSkipping line 960713: expected 15 fields, saw 22\\nSkipping line 965225: expected 15 fields, saw 22\\nSkipping line 980776: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 999318: expected 15 fields, saw 22\\nSkipping line 1007247: expected 15 fields, saw 22\\nSkipping line 1015987: expected 15 fields, saw 22\\nSkipping line 1018984: expected 15 fields, saw 22\\nSkipping line 1028671: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1063360: expected 15 fields, saw 22\\nSkipping line 1066195: expected 15 fields, saw 22\\nSkipping line 1066578: expected 15 fields, saw 22\\nSkipping line 1066869: expected 15 fields, saw 22\\nSkipping line 1068809: expected 15 fields, saw 22\\nSkipping line 1069505: expected 15 fields, saw 22\\nSkipping line 1087983: expected 15 fields, saw 22\\nSkipping line 1108184: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1118137: expected 15 fields, saw 22\\nSkipping line 1142723: expected 15 fields, saw 22\\nSkipping line 1152492: expected 15 fields, saw 22\\nSkipping line 1156947: expected 15 fields, saw 22\\nSkipping line 1172563: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1209254: expected 15 fields, saw 22\\nSkipping line 1212966: expected 15 fields, saw 22\\nSkipping line 1236533: expected 15 fields, saw 22\\nSkipping line 1237598: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1273825: expected 15 fields, saw 22\\nSkipping line 1277898: expected 15 fields, saw 22\\nSkipping line 1283654: expected 15 fields, saw 22\\nSkipping line 1286023: expected 15 fields, saw 22\\nSkipping line 1302038: expected 15 fields, saw 22\\nSkipping line 1305179: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1326022: expected 15 fields, saw 22\\nSkipping line 1338120: expected 15 fields, saw 22\\nSkipping line 1338503: expected 15 fields, saw 22\\nSkipping line 1338849: expected 15 fields, saw 22\\nSkipping line 1341513: expected 15 fields, saw 22\\nSkipping line 1346493: expected 15 fields, saw 22\\nSkipping line 1373127: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1389508: expected 15 fields, saw 22\\nSkipping line 1413951: expected 15 fields, saw 22\\nSkipping line 1433626: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1442698: expected 15 fields, saw 22\\nSkipping line 1472982: expected 15 fields, saw 22\\nSkipping line 1482282: expected 15 fields, saw 22\\nSkipping line 1487808: expected 15 fields, saw 22\\nSkipping line 1500636: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1511479: expected 15 fields, saw 22\\nSkipping line 1532302: expected 15 fields, saw 22\\nSkipping line 1537952: expected 15 fields, saw 22\\nSkipping line 1539951: expected 15 fields, saw 22\\nSkipping line 1541020: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1594217: expected 15 fields, saw 22\\nSkipping line 1612264: expected 15 fields, saw 22\\nSkipping line 1615907: expected 15 fields, saw 22\\nSkipping line 1621859: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1653542: expected 15 fields, saw 22\\nSkipping line 1671537: expected 15 fields, saw 22\\nSkipping line 1672879: expected 15 fields, saw 22\\nSkipping line 1674523: expected 15 fields, saw 22\\nSkipping line 1677355: expected 15 fields, saw 22\\nSkipping line 1703907: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1713046: expected 15 fields, saw 22\\nSkipping line 1722982: expected 15 fields, saw 22\\nSkipping line 1727290: expected 15 fields, saw 22\\nSkipping line 1744482: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1803858: expected 15 fields, saw 22\\nSkipping line 1810069: expected 15 fields, saw 22\\nSkipping line 1829751: expected 15 fields, saw 22\\nSkipping line 1831699: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1863131: expected 15 fields, saw 22\\nSkipping line 1867917: expected 15 fields, saw 22\\nSkipping line 1874790: expected 15 fields, saw 22\\nSkipping line 1879952: expected 15 fields, saw 22\\nSkipping line 1880501: expected 15 fields, saw 22\\nSkipping line 1886655: expected 15 fields, saw 22\\nSkipping line 1887888: expected 15 fields, saw 22\\nSkipping line 1894286: expected 15 fields, saw 22\\nSkipping line 1895400: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1904040: expected 15 fields, saw 22\\nSkipping line 1907604: expected 15 fields, saw 22\\nSkipping line 1915739: expected 15 fields, saw 22\\nSkipping line 1921514: expected 15 fields, saw 22\\nSkipping line 1939428: expected 15 fields, saw 22\\nSkipping line 1944342: expected 15 fields, saw 22\\nSkipping line 1949699: expected 15 fields, saw 22\\nSkipping line 1961872: expected 15 fields, saw 22\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 1968846: expected 15 fields, saw 22\\nSkipping line 1999941: expected 15 fields, saw 22\\nSkipping line 2001492: expected 15 fields, saw 22\\nSkipping line 2011204: expected 15 fields, saw 22\\nSkipping line 2025295: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2041266: expected 15 fields, saw 22\\nSkipping line 2073314: expected 15 fields, saw 22\\nSkipping line 2080133: expected 15 fields, saw 22\\nSkipping line 2088521: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2103490: expected 15 fields, saw 22\\nSkipping line 2115278: expected 15 fields, saw 22\\nSkipping line 2153174: expected 15 fields, saw 22\\nSkipping line 2161731: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2165250: expected 15 fields, saw 22\\nSkipping line 2175132: expected 15 fields, saw 22\\nSkipping line 2206817: expected 15 fields, saw 22\\nSkipping line 2215848: expected 15 fields, saw 22\\nSkipping line 2223811: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2257265: expected 15 fields, saw 22\\nSkipping line 2259163: expected 15 fields, saw 22\\nSkipping line 2263291: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2301943: expected 15 fields, saw 22\\nSkipping line 2304371: expected 15 fields, saw 22\\nSkipping line 2306015: expected 15 fields, saw 22\\nSkipping line 2312186: expected 15 fields, saw 22\\nSkipping line 2314740: expected 15 fields, saw 22\\nSkipping line 2317754: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2383514: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2449763: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2589323: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2775036: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2935174: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3078830: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3123091: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3185533: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 4150395: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 4748401: expected 15 fields, saw 22\\n'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv(\"amazon_reviews_us_Kitchen_v1_00.tsv\", sep = '\\t',error_bad_lines = False)\n",
    "test['label'] = -1\n",
    "a1 = test.loc[test['star_rating']==1].sample(50000)\n",
    "a2 = test.loc[test['star_rating']==2].sample(50000)\n",
    "a3 = test.loc[test['star_rating']==3].sample(50000)\n",
    "a4 = test.loc[test['star_rating']==4].sample(50000)\n",
    "a5 = test.loc[test['star_rating']==5].sample(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ef42a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test = pd.concat([a1,a2,a3,a4,a5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68460368",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test['lable'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e778804e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test.label[test.star_rating>3] = 1\n",
    "new_test.label[test.star_rating<3] = 2\n",
    "new_test.label[test.star_rating==3] = 3\n",
    "new_test = new_test[['label','review_body']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a164e9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model_1 = gensim.models.Word2Vec.load('save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caa53016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data clearing\n",
    "new_test['review_body'] = new_test['review_body'].str.lower()\n",
    "\n",
    "def tag(x):\n",
    "    return re.sub('<.*?>','',str(x))\n",
    "new_test['review_body'] = new_test['review_body'].apply(lambda x:tag(x))\n",
    "\n",
    "def url(x):\n",
    "    return re.sub('(https?|ftp|file)://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]','',str(x))\n",
    "\n",
    "new_test['review_body'] = new_test['review_body'].apply(lambda x:url(x))\n",
    "\n",
    "import contractions\n",
    "new_test['review_body'] = new_test['review_body'].apply(lambda x:contractions.fix(x))\n",
    "\n",
    "def non_alphabetical(x):\n",
    "    return re.sub('[^a-zA-Z\\s]','',str(x))\n",
    "new_test['review_body'] = new_test['review_body'].apply(lambda x:non_alphabetical(x))\n",
    "def extra_space(x):\n",
    "    return re.sub( ' +',' ',str(x))\n",
    "new_test['review_body'] = new_test['review_body'].apply(lambda x:extra_space(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed417460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chenlin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words_set = set(stopwords.words('english'))\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def stop_words(x):\n",
    "    word_tokens = word_tokenize(x)\n",
    "    temp = []\n",
    "    for i in word_tokens:\n",
    "        if i not in stop_words_set:\n",
    "            temp.append(i)\n",
    "    return temp\n",
    "new_test['review_body'] = new_test['review_body'].apply(lambda x:stop_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4cd4c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test['y'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd26812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test['y'][new_test['label'] == 1] =0\n",
    "new_test['y'][new_test['label'] == 2] =1\n",
    "new_test['y'][new_test['label'] == 3] =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "195645b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = new_test.loc[new_test['y']!=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b3da25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mymodel_reviewtensor(x):\n",
    "    max_review = 50\n",
    "    review = torch.zeros(max_review,1,300)\n",
    "    pad = torch.zeros(300)\n",
    "    index = 0\n",
    "    for i in x:\n",
    "        if i in model_1.wv and index < 50:\n",
    "            vector = model_1.wv [i]\n",
    "            review[index][0] = torch.from_numpy(vector)\n",
    "            index += 1\n",
    "        else:\n",
    "            continue\n",
    "    if index < 50:\n",
    "        review[index][0] = pad\n",
    "    review = torch.nan_to_num(review)\n",
    "    return review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "441b175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model_1 = gensim.models.Word2Vec.load('save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2148ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "opm = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42973d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opm_reviewtensor(x):\n",
    "    max_review = 50\n",
    "    ### create a matrix \n",
    "    review = torch.zeros(max_review,1,300)\n",
    "    pad = torch.zeros(300)\n",
    "    index = 0\n",
    "    for i in x:\n",
    "        if i in opm and index < 50:\n",
    "            vector = opm [i]\n",
    "            review[index][0] = torch.from_numpy(vector)\n",
    "            index += 1\n",
    "        else:\n",
    "            continue\n",
    "    if index < 50:\n",
    "        review[index][0] = pad\n",
    "    review = torch.nan_to_num(review)\n",
    "    return review \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "329ae5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_vector(dataset,opm):\n",
    "    res = []\n",
    "    if opm == True:\n",
    "        for x,y in dataset:\n",
    "            x_vector = opm_reviewtensor(x)\n",
    "            res.append((x_vector,y))\n",
    "    if opm == False:\n",
    "        for x,y in dataset:\n",
    "            x_vector = mymodel_reviewtensor(x)\n",
    "            res.append((x_vector,y))\n",
    "    return res\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a62c9499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review_body</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3840037</th>\n",
       "      <td>2</td>\n",
       "      <td>[one, highly, disappointed, product, years, wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3791453</th>\n",
       "      <td>2</td>\n",
       "      <td>[bought, month, ago, bbb, times, use, make, si...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2539564</th>\n",
       "      <td>2</td>\n",
       "      <td>[bought, brewer, jan, within, weeks, carafe, e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3239783</th>\n",
       "      <td>2</td>\n",
       "      <td>[bought, couple, clean, bottom, beta, fish, bo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2740932</th>\n",
       "      <td>2</td>\n",
       "      <td>[love, productbut, old, one, broke, wanted, ge...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2183853</th>\n",
       "      <td>1</td>\n",
       "      <td>[blast, family, making, taking, pics]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020136</th>\n",
       "      <td>1</td>\n",
       "      <td>[great, purchase, please, came, time, loved]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758315</th>\n",
       "      <td>1</td>\n",
       "      <td>[perfect, glasses, love]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175724</th>\n",
       "      <td>1</td>\n",
       "      <td>[pans, first, quality, mediumheavy, gauge, sta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4038384</th>\n",
       "      <td>1</td>\n",
       "      <td>[cup, amazing, shipped, accurately, says, cano...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                        review_body  y\n",
       "3840037      2  [one, highly, disappointed, product, years, wa...  1\n",
       "3791453      2  [bought, month, ago, bbb, times, use, make, si...  1\n",
       "2539564      2  [bought, brewer, jan, within, weeks, carafe, e...  1\n",
       "3239783      2  [bought, couple, clean, bottom, beta, fish, bo...  1\n",
       "2740932      2  [love, productbut, old, one, broke, wanted, ge...  1\n",
       "...        ...                                                ... ..\n",
       "2183853      1              [blast, family, making, taking, pics]  0\n",
       "1020136      1       [great, purchase, please, came, time, loved]  0\n",
       "1758315      1                           [perfect, glasses, love]  0\n",
       "4175724      1  [pans, first, quality, mediumheavy, gauge, sta...  0\n",
       "4038384      1  [cup, amazing, shipped, accurately, says, cano...  0\n",
       "\n",
       "[200000 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fc2d7d",
   "metadata": {},
   "source": [
    "## binary GRU my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77474eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_data = list(zip(binary.review_body.values,binary.y.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4826f8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(binary_data, random_state = 19, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e0792b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data_vector(test_set, opm=False )\n",
    "\n",
    "train = data_vector(train_set, opm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d49d67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68343c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 2\n",
    "# how many samples per batch to load\n",
    "batch_size = 100\n",
    "# percentage of training set to use as validation\n",
    "#valid_size = 0.2\n",
    "train_loader=torch.utils.data.DataLoader(train,batch_size=batch_size,shuffle=True,drop_last=True, num_workers=0)\n",
    "\n",
    "test_loader=torch.utils.data.DataLoader(test, batch_size=batch_size, drop_last=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e185bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(GRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        \n",
    "        \n",
    "        out, _ = self.gru(x, h0)  \n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "GRU_model = GRU(300, 50, 2, 2).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "703c695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(GRU_model.parameters(), lr=0.002)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ab10da51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [100/1600], Loss: 0.5400\n",
      "Epoch [1/1], Step [200/1600], Loss: 0.4260\n",
      "Epoch [1/1], Step [300/1600], Loss: 0.2985\n",
      "Epoch [1/1], Step [400/1600], Loss: 0.2918\n",
      "Epoch [1/1], Step [500/1600], Loss: 0.3736\n",
      "Epoch [1/1], Step [600/1600], Loss: 0.3736\n",
      "Epoch [1/1], Step [700/1600], Loss: 0.2612\n",
      "Epoch [1/1], Step [800/1600], Loss: 0.3358\n",
      "Epoch [1/1], Step [900/1600], Loss: 0.2936\n",
      "Epoch [1/1], Step [1000/1600], Loss: 0.3211\n",
      "Epoch [1/1], Step [1100/1600], Loss: 0.3775\n",
      "Epoch [1/1], Step [1200/1600], Loss: 0.3397\n",
      "Epoch [1/1], Step [1300/1600], Loss: 0.4303\n",
      "Epoch [1/1], Step [1400/1600], Loss: 0.2620\n",
      "Epoch [1/1], Step [1500/1600], Loss: 0.3403\n",
      "Epoch [1/1], Step [1600/1600], Loss: 0.2838\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 1\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (x_i,y_i) in enumerate(train_loader):  \n",
    "        # origin shape: [N, 1, 28, 28]\n",
    "        # resized: [N, 28, 28]\n",
    "        x_i = x_i.reshape(-1, 50, 300).to(device)\n",
    "        y_i = y_i.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = GRU_model(x_i)\n",
    "        loss = criterion(outputs, y_i)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6d571e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 87.6475 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 50, 300).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = GRU_model(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of binary GRU my model: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "37fbee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_GRU_my_model = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "202ec976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of binary GRU my model: 87.6475 %\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy of binary GRU my model: {acc} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0845640",
   "metadata": {},
   "source": [
    "# binary GRU googel model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "430ed2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data_vector(test_set, opm=True )\n",
    "\n",
    "train = data_vector(train_set, opm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e7a6dd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 2\n",
    "# how many samples per batch to load\n",
    "batch_size = 100\n",
    "# percentage of training set to use as validation\n",
    "#valid_size = 0.2\n",
    "train_loader=torch.utils.data.DataLoader(train,batch_size=batch_size,shuffle=True,drop_last=True, num_workers=0)\n",
    "\n",
    "test_loader=torch.utils.data.DataLoader(test, batch_size=batch_size, drop_last=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dee04caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(GRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        \n",
    "        \n",
    "        out, _ = self.gru(x, h0)  \n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "GRU_model = GRU(300, 50, 2, 2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7c82ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(GRU_model.parameters(), lr=0.002)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1db821a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [100/1600], Loss: 0.6452\n",
      "Epoch [1/1], Step [200/1600], Loss: 0.5023\n",
      "Epoch [1/1], Step [300/1600], Loss: 0.4549\n",
      "Epoch [1/1], Step [400/1600], Loss: 0.3687\n",
      "Epoch [1/1], Step [500/1600], Loss: 0.3416\n",
      "Epoch [1/1], Step [600/1600], Loss: 0.3899\n",
      "Epoch [1/1], Step [700/1600], Loss: 0.3783\n",
      "Epoch [1/1], Step [800/1600], Loss: 0.4301\n",
      "Epoch [1/1], Step [900/1600], Loss: 0.3203\n",
      "Epoch [1/1], Step [1000/1600], Loss: 0.3960\n",
      "Epoch [1/1], Step [1100/1600], Loss: 0.3546\n",
      "Epoch [1/1], Step [1200/1600], Loss: 0.3367\n",
      "Epoch [1/1], Step [1300/1600], Loss: 0.3432\n",
      "Epoch [1/1], Step [1400/1600], Loss: 0.3284\n",
      "Epoch [1/1], Step [1500/1600], Loss: 0.3906\n",
      "Epoch [1/1], Step [1600/1600], Loss: 0.3222\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 1\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (x_i,y_i) in enumerate(train_loader):  \n",
    "        # origin shape: [N, 1, 28, 28]\n",
    "        # resized: [N, 28, 28]\n",
    "        x_i = x_i.reshape(-1, 50, 300).to(device)\n",
    "        y_i = y_i.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = GRU_model(x_i)\n",
    "        loss = criterion(outputs, y_i)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3480ede9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of binary GRU google model: 87.3825 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 50, 300).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = GRU_model(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of binary GRU google model: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0ad94461",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_GRU_opm = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdf98b8",
   "metadata": {},
   "source": [
    "# Ternary GRU my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3a500552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [100/2000], Loss: 0.9219\n",
      "Epoch [1/1], Step [200/2000], Loss: 0.7806\n",
      "Epoch [1/1], Step [300/2000], Loss: 0.6303\n",
      "Epoch [1/1], Step [400/2000], Loss: 0.6929\n",
      "Epoch [1/1], Step [500/2000], Loss: 0.7986\n",
      "Epoch [1/1], Step [600/2000], Loss: 0.7141\n",
      "Epoch [1/1], Step [700/2000], Loss: 0.6560\n",
      "Epoch [1/1], Step [800/2000], Loss: 0.6516\n",
      "Epoch [1/1], Step [900/2000], Loss: 0.6664\n",
      "Epoch [1/1], Step [1000/2000], Loss: 0.7144\n",
      "Epoch [1/1], Step [1100/2000], Loss: 0.7327\n",
      "Epoch [1/1], Step [1200/2000], Loss: 0.6977\n",
      "Epoch [1/1], Step [1300/2000], Loss: 0.7030\n",
      "Epoch [1/1], Step [1400/2000], Loss: 0.6500\n",
      "Epoch [1/1], Step [1500/2000], Loss: 0.6327\n",
      "Epoch [1/1], Step [1600/2000], Loss: 0.6348\n",
      "Epoch [1/1], Step [1700/2000], Loss: 0.7318\n",
      "Epoch [1/1], Step [1800/2000], Loss: 0.7805\n",
      "Epoch [1/1], Step [1900/2000], Loss: 0.6831\n",
      "Epoch [1/1], Step [2000/2000], Loss: 0.6341\n"
     ]
    }
   ],
   "source": [
    "ternary_data = list(zip(new_test.review_body.values,new_test.y.values))\n",
    "\n",
    "train_set, test_set = train_test_split(ternary_data, random_state = 19, test_size = 0.2)\n",
    "\n",
    "test = data_vector(test_set, opm=False )\n",
    "\n",
    "train = data_vector(train_set, opm=False)\n",
    "\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 2\n",
    "# how many samples per batch to load\n",
    "batch_size = 100\n",
    "# percentage of training set to use as validation\n",
    "#valid_size = 0.2\n",
    "train_loader=torch.utils.data.DataLoader(train,batch_size=batch_size,shuffle=True,drop_last=True, num_workers=0)\n",
    "\n",
    "test_loader=torch.utils.data.DataLoader(test, batch_size=batch_size, drop_last=True,num_workers=0)\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(GRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        \n",
    "        \n",
    "        out, _ = self.gru(x, h0)  \n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "GRU_model = GRU(300, 50, 2, 3).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(GRU_model.parameters(), lr=0.002)  \n",
    "\n",
    "# Train the model\n",
    "num_epochs = 1\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (x_i,y_i) in enumerate(train_loader):  \n",
    "        # origin shape: [N, 1, 28, 28]\n",
    "        # resized: [N, 28, 28]\n",
    "        x_i = x_i.reshape(-1, 50, 300).to(device)\n",
    "        y_i = y_i.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = GRU_model(x_i)\n",
    "        loss = criterion(outputs, y_i)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0fc2e0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of ternary GRU my model: 72.188 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 50, 300).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = GRU_model(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of ternary GRU my model: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8869869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_GRU_my_model = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f0a125f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72.188"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_GRU_my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d626c811",
   "metadata": {},
   "source": [
    "# Ternary GRU google model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "64006669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [100/2000], Loss: 0.9518\n",
      "Epoch [1/1], Step [200/2000], Loss: 0.8481\n",
      "Epoch [1/1], Step [300/2000], Loss: 0.7337\n",
      "Epoch [1/1], Step [400/2000], Loss: 0.8770\n",
      "Epoch [1/1], Step [500/2000], Loss: 0.7802\n",
      "Epoch [1/1], Step [600/2000], Loss: 0.7466\n",
      "Epoch [1/1], Step [700/2000], Loss: 0.8227\n",
      "Epoch [1/1], Step [800/2000], Loss: 0.7063\n",
      "Epoch [1/1], Step [900/2000], Loss: 0.7319\n",
      "Epoch [1/1], Step [1000/2000], Loss: 0.6627\n",
      "Epoch [1/1], Step [1100/2000], Loss: 0.6182\n",
      "Epoch [1/1], Step [1200/2000], Loss: 0.7480\n",
      "Epoch [1/1], Step [1300/2000], Loss: 0.6651\n",
      "Epoch [1/1], Step [1400/2000], Loss: 0.6778\n",
      "Epoch [1/1], Step [1500/2000], Loss: 0.6995\n",
      "Epoch [1/1], Step [1600/2000], Loss: 0.6357\n",
      "Epoch [1/1], Step [1700/2000], Loss: 0.5874\n",
      "Epoch [1/1], Step [1800/2000], Loss: 0.6396\n",
      "Epoch [1/1], Step [1900/2000], Loss: 0.6808\n",
      "Epoch [1/1], Step [2000/2000], Loss: 0.7171\n"
     ]
    }
   ],
   "source": [
    "ternary_data = list(zip(new_test.review_body.values,new_test.y.values))\n",
    "\n",
    "train_set, test_set = train_test_split(ternary_data, random_state = 19, test_size = 0.2)\n",
    "\n",
    "test = data_vector(test_set, opm=True )\n",
    "\n",
    "train = data_vector(train_set, opm=True)\n",
    "\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 2\n",
    "# how many samples per batch to load\n",
    "batch_size = 100\n",
    "# percentage of training set to use as validation\n",
    "#valid_size = 0.2\n",
    "train_loader=torch.utils.data.DataLoader(train,batch_size=batch_size,shuffle=True,drop_last=True, num_workers=0)\n",
    "\n",
    "test_loader=torch.utils.data.DataLoader(test, batch_size=batch_size, drop_last=True,num_workers=0)\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(GRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        \n",
    "        \n",
    "        out, _ = self.gru(x, h0)  \n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "GRU_model = GRU(300, 50, 2, 3).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(GRU_model.parameters(), lr=0.002)  \n",
    "\n",
    "# Train the model\n",
    "num_epochs = 1\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (x_i,y_i) in enumerate(train_loader):  \n",
    "        # origin shape: [N, 1, 28, 28]\n",
    "        # resized: [N, 28, 28]\n",
    "        x_i = x_i.reshape(-1, 50, 300).to(device)\n",
    "        y_i = y_i.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = GRU_model(x_i)\n",
    "        loss = criterion(outputs, y_i)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d415b13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of ternary GRU google model: 71.422 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 50, 300).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = GRU_model(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of ternary GRU google model: {acc} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e88bc16",
   "metadata": {},
   "source": [
    "## GRU conclusion\n",
    "\n",
    "Accuracy of ternary GRU google model: 71.422 %\n",
    "\n",
    "Accuracy of ternary GRU my model: 72.188 %\n",
    "\n",
    "Accuracy of binary GRU google model: 87.3825 %\n",
    "\n",
    "Accuracy of binary GRU my model: 87.6475 %"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
