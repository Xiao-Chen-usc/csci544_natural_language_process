{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fc568df",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba0755f",
   "metadata": {},
   "source": [
    "## binary with my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "db8d734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f95d2d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72fab82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 16148: expected 15 fields, saw 22\\nSkipping line 20100: expected 15 fields, saw 22\\nSkipping line 45178: expected 15 fields, saw 22\\nSkipping line 48700: expected 15 fields, saw 22\\nSkipping line 63331: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 86053: expected 15 fields, saw 22\\nSkipping line 88858: expected 15 fields, saw 22\\nSkipping line 115017: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 137366: expected 15 fields, saw 22\\nSkipping line 139110: expected 15 fields, saw 22\\nSkipping line 165540: expected 15 fields, saw 22\\nSkipping line 171813: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 203723: expected 15 fields, saw 22\\nSkipping line 209366: expected 15 fields, saw 22\\nSkipping line 211310: expected 15 fields, saw 22\\nSkipping line 246351: expected 15 fields, saw 22\\nSkipping line 252364: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 267003: expected 15 fields, saw 22\\nSkipping line 268957: expected 15 fields, saw 22\\nSkipping line 303336: expected 15 fields, saw 22\\nSkipping line 306021: expected 15 fields, saw 22\\nSkipping line 311569: expected 15 fields, saw 22\\nSkipping line 316767: expected 15 fields, saw 22\\nSkipping line 324009: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 359107: expected 15 fields, saw 22\\nSkipping line 368367: expected 15 fields, saw 22\\nSkipping line 381180: expected 15 fields, saw 22\\nSkipping line 390453: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 412243: expected 15 fields, saw 22\\nSkipping line 419342: expected 15 fields, saw 22\\nSkipping line 457388: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 459935: expected 15 fields, saw 22\\nSkipping line 460167: expected 15 fields, saw 22\\nSkipping line 466460: expected 15 fields, saw 22\\nSkipping line 500314: expected 15 fields, saw 22\\nSkipping line 500339: expected 15 fields, saw 22\\nSkipping line 505396: expected 15 fields, saw 22\\nSkipping line 507760: expected 15 fields, saw 22\\nSkipping line 513626: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 527638: expected 15 fields, saw 22\\nSkipping line 534209: expected 15 fields, saw 22\\nSkipping line 535687: expected 15 fields, saw 22\\nSkipping line 547671: expected 15 fields, saw 22\\nSkipping line 549054: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 599929: expected 15 fields, saw 22\\nSkipping line 604776: expected 15 fields, saw 22\\nSkipping line 609937: expected 15 fields, saw 22\\nSkipping line 632059: expected 15 fields, saw 22\\nSkipping line 638546: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 665017: expected 15 fields, saw 22\\nSkipping line 677680: expected 15 fields, saw 22\\nSkipping line 684370: expected 15 fields, saw 22\\nSkipping line 720217: expected 15 fields, saw 29\\n'\n",
      "b'Skipping line 723240: expected 15 fields, saw 22\\nSkipping line 723433: expected 15 fields, saw 22\\nSkipping line 763891: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 800288: expected 15 fields, saw 22\\nSkipping line 802942: expected 15 fields, saw 22\\nSkipping line 803379: expected 15 fields, saw 22\\nSkipping line 805122: expected 15 fields, saw 22\\nSkipping line 821899: expected 15 fields, saw 22\\nSkipping line 831707: expected 15 fields, saw 22\\nSkipping line 842829: expected 15 fields, saw 22\\nSkipping line 843604: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 863904: expected 15 fields, saw 22\\nSkipping line 875655: expected 15 fields, saw 22\\nSkipping line 886796: expected 15 fields, saw 22\\nSkipping line 892299: expected 15 fields, saw 22\\nSkipping line 902518: expected 15 fields, saw 22\\nSkipping line 903079: expected 15 fields, saw 22\\nSkipping line 912678: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 932953: expected 15 fields, saw 22\\nSkipping line 936838: expected 15 fields, saw 22\\nSkipping line 937177: expected 15 fields, saw 22\\nSkipping line 947695: expected 15 fields, saw 22\\nSkipping line 960713: expected 15 fields, saw 22\\nSkipping line 965225: expected 15 fields, saw 22\\nSkipping line 980776: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 999318: expected 15 fields, saw 22\\nSkipping line 1007247: expected 15 fields, saw 22\\nSkipping line 1015987: expected 15 fields, saw 22\\nSkipping line 1018984: expected 15 fields, saw 22\\nSkipping line 1028671: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1063360: expected 15 fields, saw 22\\nSkipping line 1066195: expected 15 fields, saw 22\\nSkipping line 1066578: expected 15 fields, saw 22\\nSkipping line 1066869: expected 15 fields, saw 22\\nSkipping line 1068809: expected 15 fields, saw 22\\nSkipping line 1069505: expected 15 fields, saw 22\\nSkipping line 1087983: expected 15 fields, saw 22\\nSkipping line 1108184: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1118137: expected 15 fields, saw 22\\nSkipping line 1142723: expected 15 fields, saw 22\\nSkipping line 1152492: expected 15 fields, saw 22\\nSkipping line 1156947: expected 15 fields, saw 22\\nSkipping line 1172563: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1209254: expected 15 fields, saw 22\\nSkipping line 1212966: expected 15 fields, saw 22\\nSkipping line 1236533: expected 15 fields, saw 22\\nSkipping line 1237598: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1273825: expected 15 fields, saw 22\\nSkipping line 1277898: expected 15 fields, saw 22\\nSkipping line 1283654: expected 15 fields, saw 22\\nSkipping line 1286023: expected 15 fields, saw 22\\nSkipping line 1302038: expected 15 fields, saw 22\\nSkipping line 1305179: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1326022: expected 15 fields, saw 22\\nSkipping line 1338120: expected 15 fields, saw 22\\nSkipping line 1338503: expected 15 fields, saw 22\\nSkipping line 1338849: expected 15 fields, saw 22\\nSkipping line 1341513: expected 15 fields, saw 22\\nSkipping line 1346493: expected 15 fields, saw 22\\nSkipping line 1373127: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1389508: expected 15 fields, saw 22\\nSkipping line 1413951: expected 15 fields, saw 22\\nSkipping line 1433626: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1442698: expected 15 fields, saw 22\\nSkipping line 1472982: expected 15 fields, saw 22\\nSkipping line 1482282: expected 15 fields, saw 22\\nSkipping line 1487808: expected 15 fields, saw 22\\nSkipping line 1500636: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1511479: expected 15 fields, saw 22\\nSkipping line 1532302: expected 15 fields, saw 22\\nSkipping line 1537952: expected 15 fields, saw 22\\nSkipping line 1539951: expected 15 fields, saw 22\\nSkipping line 1541020: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1594217: expected 15 fields, saw 22\\nSkipping line 1612264: expected 15 fields, saw 22\\nSkipping line 1615907: expected 15 fields, saw 22\\nSkipping line 1621859: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1653542: expected 15 fields, saw 22\\nSkipping line 1671537: expected 15 fields, saw 22\\nSkipping line 1672879: expected 15 fields, saw 22\\nSkipping line 1674523: expected 15 fields, saw 22\\nSkipping line 1677355: expected 15 fields, saw 22\\nSkipping line 1703907: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1713046: expected 15 fields, saw 22\\nSkipping line 1722982: expected 15 fields, saw 22\\nSkipping line 1727290: expected 15 fields, saw 22\\nSkipping line 1744482: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1803858: expected 15 fields, saw 22\\nSkipping line 1810069: expected 15 fields, saw 22\\nSkipping line 1829751: expected 15 fields, saw 22\\nSkipping line 1831699: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1863131: expected 15 fields, saw 22\\nSkipping line 1867917: expected 15 fields, saw 22\\nSkipping line 1874790: expected 15 fields, saw 22\\nSkipping line 1879952: expected 15 fields, saw 22\\nSkipping line 1880501: expected 15 fields, saw 22\\nSkipping line 1886655: expected 15 fields, saw 22\\nSkipping line 1887888: expected 15 fields, saw 22\\nSkipping line 1894286: expected 15 fields, saw 22\\nSkipping line 1895400: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1904040: expected 15 fields, saw 22\\nSkipping line 1907604: expected 15 fields, saw 22\\nSkipping line 1915739: expected 15 fields, saw 22\\nSkipping line 1921514: expected 15 fields, saw 22\\nSkipping line 1939428: expected 15 fields, saw 22\\nSkipping line 1944342: expected 15 fields, saw 22\\nSkipping line 1949699: expected 15 fields, saw 22\\nSkipping line 1961872: expected 15 fields, saw 22\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 1968846: expected 15 fields, saw 22\\nSkipping line 1999941: expected 15 fields, saw 22\\nSkipping line 2001492: expected 15 fields, saw 22\\nSkipping line 2011204: expected 15 fields, saw 22\\nSkipping line 2025295: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2041266: expected 15 fields, saw 22\\nSkipping line 2073314: expected 15 fields, saw 22\\nSkipping line 2080133: expected 15 fields, saw 22\\nSkipping line 2088521: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2103490: expected 15 fields, saw 22\\nSkipping line 2115278: expected 15 fields, saw 22\\nSkipping line 2153174: expected 15 fields, saw 22\\nSkipping line 2161731: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2165250: expected 15 fields, saw 22\\nSkipping line 2175132: expected 15 fields, saw 22\\nSkipping line 2206817: expected 15 fields, saw 22\\nSkipping line 2215848: expected 15 fields, saw 22\\nSkipping line 2223811: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2257265: expected 15 fields, saw 22\\nSkipping line 2259163: expected 15 fields, saw 22\\nSkipping line 2263291: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2301943: expected 15 fields, saw 22\\nSkipping line 2304371: expected 15 fields, saw 22\\nSkipping line 2306015: expected 15 fields, saw 22\\nSkipping line 2312186: expected 15 fields, saw 22\\nSkipping line 2314740: expected 15 fields, saw 22\\nSkipping line 2317754: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2383514: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2449763: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2589323: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2775036: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2935174: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3078830: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3123091: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3185533: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 4150395: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 4748401: expected 15 fields, saw 22\\n'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv(\"amazon_reviews_us_Kitchen_v1_00.tsv\", sep = '\\t',error_bad_lines = False)\n",
    "test['label'] = -1\n",
    "a1 = test.loc[test['star_rating']==1].sample(50000)\n",
    "a2 = test.loc[test['star_rating']==2].sample(50000)\n",
    "a3 = test.loc[test['star_rating']==3].sample(50000)\n",
    "a4 = test.loc[test['star_rating']==4].sample(50000)\n",
    "a5 = test.loc[test['star_rating']==5].sample(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f312e264",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test = pd.concat([a1,a2,a3,a4,a5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6d26b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test['lable'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "491523b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "new_test.label[test.star_rating>3] = 1\n",
    "new_test.label[test.star_rating<3] = 2\n",
    "new_test.label[test.star_rating==3] = 3\n",
    "new_test = new_test[['label','review_body']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f629760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model_1 = gensim.models.Word2Vec.load('save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49be904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data clearing\n",
    "new_test['review_body'] = new_test['review_body'].str.lower()\n",
    "\n",
    "def tag(x):\n",
    "    return re.sub('<.*?>','',str(x))\n",
    "new_test['review_body'] = new_test['review_body'].apply(lambda x:tag(x))\n",
    "\n",
    "def url(x):\n",
    "    return re.sub('(https?|ftp|file)://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]','',str(x))\n",
    "\n",
    "new_test['review_body'] = new_test['review_body'].apply(lambda x:url(x))\n",
    "\n",
    "import contractions\n",
    "new_test['review_body'] = new_test['review_body'].apply(lambda x:contractions.fix(x))\n",
    "\n",
    "def non_alphabetical(x):\n",
    "    return re.sub('[^a-zA-Z\\s]','',str(x))\n",
    "new_test['review_body'] = new_test['review_body'].apply(lambda x:non_alphabetical(x))\n",
    "def extra_space(x):\n",
    "    return re.sub( ' +',' ',str(x))\n",
    "new_test['review_body'] = new_test['review_body'].apply(lambda x:extra_space(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "155845b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chenlin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words_set = set(stopwords.words('english'))\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def stop_words(x):\n",
    "    word_tokens = word_tokenize(x)\n",
    "    temp = []\n",
    "    for i in word_tokens:\n",
    "        if i not in stop_words_set:\n",
    "            temp.append(i)\n",
    "    return temp\n",
    "new_test['review_body'] = new_test['review_body'].apply(lambda x:stop_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cbfe552",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test['y'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73a1b37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "new_test['y'][new_test['label'] == 1] =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de12b332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "new_test['y'][new_test['label'] == 2] =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc187e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "new_test['y'][new_test['label'] == 3] =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "915caa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = new_test.loc[new_test['y']!=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90c91708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review_body</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1674978</th>\n",
       "      <td>2</td>\n",
       "      <td>[wrong, sizehad, return]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3055998</th>\n",
       "      <td>2</td>\n",
       "      <td>[bought, replace, old, mandolins, slicer, cut,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4165773</th>\n",
       "      <td>2</td>\n",
       "      <td>[looking, forward, getting, handy, new, kitche...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2168375</th>\n",
       "      <td>2</td>\n",
       "      <td>[item, cheap, could, made, paper, inside, croo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2627653</th>\n",
       "      <td>2</td>\n",
       "      <td>[got, year, old, boyfriend, awesome, girlfrien...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4171764</th>\n",
       "      <td>1</td>\n",
       "      <td>[made, amazing, popcorn, hot, air, corn, poppe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4272127</th>\n",
       "      <td>1</td>\n",
       "      <td>[bought, bagel, slicer, kids, would, cut, slic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302570</th>\n",
       "      <td>1</td>\n",
       "      <td>[great, deal]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2914686</th>\n",
       "      <td>1</td>\n",
       "      <td>[original, quality, quart, bowl, kitchenaid, r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3063914</th>\n",
       "      <td>1</td>\n",
       "      <td>[straws, great, little, kids, keep, chewing, c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                        review_body  y\n",
       "1674978      2                           [wrong, sizehad, return]  1\n",
       "3055998      2  [bought, replace, old, mandolins, slicer, cut,...  1\n",
       "4165773      2  [looking, forward, getting, handy, new, kitche...  1\n",
       "2168375      2  [item, cheap, could, made, paper, inside, croo...  1\n",
       "2627653      2  [got, year, old, boyfriend, awesome, girlfrien...  1\n",
       "...        ...                                                ... ..\n",
       "4171764      1  [made, amazing, popcorn, hot, air, corn, poppe...  0\n",
       "4272127      1  [bought, bagel, slicer, kids, would, cut, slic...  0\n",
       "1302570      1                                      [great, deal]  0\n",
       "2914686      1  [original, quality, quart, bowl, kitchenaid, r...  0\n",
       "3063914      1  [straws, great, little, kids, keep, chewing, c...  0\n",
       "\n",
       "[200000 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "fa95c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mymodel_reviewtensor(x):\n",
    "    max_review = 50\n",
    "    ### create a matrix \n",
    "    review = torch.zeros(max_review,1,300)\n",
    "    pad = torch.zeros(300)\n",
    "    index = 0\n",
    "    for i in x:\n",
    "        if i in model_1.wv and index < 50:\n",
    "            vector = model_1.wv [i]\n",
    "            review[index][0] = torch.from_numpy(vector)\n",
    "            index += 1\n",
    "        else:\n",
    "            continue\n",
    "    if index < 50:\n",
    "        review[index][0] = pad\n",
    "    review = torch.nan_to_num(review)\n",
    "    return review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "2d60a78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X = binary['review_body']\n",
    "Y = binary['y']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state = 19, test_size = 0.2)\n",
    "\n",
    "x_train = np.array(x_train.tolist())\n",
    "\n",
    "y_train = np.array(y_train.tolist())\n",
    "\n",
    "x_train = np.nan_to_num(x_train)\n",
    "\n",
    "x_test = np.array(x_test.tolist())\n",
    "\n",
    "y_test = np.array(y_test.tolist())\n",
    "\n",
    "x_test = np.nan_to_num(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "77f59cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "83c099f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "rnn = RNN(300, 50, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "55194964",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(rnn.parameters(),0.5)\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ef8db70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = mymodel_reviewtensor(binary['review_body'].iloc[4])\n",
    "hidden = torch.zeros(1, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "089d26ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_predict_y(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    y = top_i[0].item()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "fce1b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "b4ea2c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data = list(zip(x_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "03fadbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 current avg lose is  0.6935066653609275 time is  18.736644983291626\n",
      "5000 current avg lose is  0.6938000483036041 time is  39.694331884384155\n",
      "7500 current avg lose is  0.6940343046387036 time is  59.080775022506714\n",
      "10000 current avg lose is  0.6943283890724182 time is  75.36496996879578\n",
      "12500 current avg lose is  0.6985913027977944 time is  93.71864986419678\n",
      "15000 current avg lose is  0.706857127382358 time is  113.98635816574097\n",
      "17500 current avg lose is  0.7157444509387016 time is  133.35544085502625\n",
      "20000 current avg lose is  0.719268296802044 time is  151.9071388244629\n",
      "22500 current avg lose is  0.7231831370088789 time is  167.7384889125824\n",
      "25000 current avg lose is  0.7257247288429737 time is  185.27695298194885\n",
      "27500 current avg lose is  0.7280170371738347 time is  204.13538002967834\n",
      "30000 current avg lose is  0.7308424241900444 time is  220.9835648536682\n",
      "32500 current avg lose is  0.7335039394947199 time is  237.65777802467346\n",
      "35000 current avg lose is  0.736566359599999 time is  254.8173429965973\n",
      "37500 current avg lose is  0.7401764461016654 time is  273.89567017555237\n",
      "40000 current avg lose is  0.7440658328901977 time is  291.83909583091736\n",
      "42500 current avg lose is  0.7491485424865695 time is  311.85006403923035\n",
      "45000 current avg lose is  0.7534946585579051 time is  328.0279891490936\n",
      "47500 current avg lose is  0.7661504434654587 time is  350.39413690567017\n",
      "50000 current avg lose is  0.8168986825359081 time is  373.1792631149292\n",
      "52500 current avg lose is  0.8665489911493953 time is  389.9557740688324\n",
      "55000 current avg lose is  0.9174279366305808 time is  406.3925199508667\n",
      "57500 current avg lose is  0.966856176473939 time is  423.01318311691284\n",
      "60000 current avg lose is  1.0062675636826433 time is  444.79478883743286\n",
      "62500 current avg lose is  1.0448091871641731 time is  461.1006848812103\n",
      "65000 current avg lose is  1.0810720270097345 time is  477.6718201637268\n",
      "67500 current avg lose is  1.108496844244045 time is  493.89629912376404\n",
      "70000 current avg lose is  1.1319343567918965 time is  510.42331409454346\n",
      "72500 current avg lose is  1.153895557646119 time is  527.0030469894409\n",
      "75000 current avg lose is  1.173600270377148 time is  543.5604491233826\n",
      "77500 current avg lose is  1.1896666699030212 time is  560.0479960441589\n",
      "80000 current avg lose is  1.2016800544987478 time is  577.5987119674683\n",
      "82500 current avg lose is  1.2178468038481534 time is  594.5318260192871\n",
      "85000 current avg lose is  1.230443255828 time is  612.9297821521759\n",
      "87500 current avg lose is  1.242328551959492 time is  629.5187909603119\n",
      "90000 current avg lose is  1.2475180744281704 time is  646.3995950222015\n",
      "92500 current avg lose is  1.2558322203072503 time is  662.9913151264191\n",
      "95000 current avg lose is  1.264708187838483 time is  679.7814490795135\n",
      "97500 current avg lose is  1.2715983043432477 time is  696.2957010269165\n",
      "100000 current avg lose is  1.278894634093908 time is  712.9258980751038\n",
      "102500 current avg lose is  1.2852110577896705 time is  736.9921770095825\n",
      "105000 current avg lose is  1.288830072370764 time is  757.801276922226\n",
      "107500 current avg lose is  1.2934220022469416 time is  774.7577559947968\n",
      "110000 current avg lose is  1.297304562862478 time is  791.1045119762421\n",
      "112500 current avg lose is  1.3010337249147075 time is  806.8263080120087\n",
      "115000 current avg lose is  1.305661886688165 time is  823.0702760219574\n",
      "117500 current avg lose is  1.308220576642449 time is  839.2411432266235\n",
      "120000 current avg lose is  1.309006782510335 time is  855.0096199512482\n",
      "122500 current avg lose is  1.3090750498813901 time is  871.5667219161987\n",
      "125000 current avg lose is  1.3101933612420882 time is  890.1921060085297\n",
      "127500 current avg lose is  1.3123350381581371 time is  908.104542016983\n",
      "130000 current avg lose is  1.3130587492290975 time is  926.8979659080505\n",
      "132500 current avg lose is  1.3161465093867415 time is  946.0703160762787\n",
      "135000 current avg lose is  1.3170001468154895 time is  965.9388539791107\n",
      "137500 current avg lose is  1.3180350640610174 time is  984.0711009502411\n",
      "140000 current avg lose is  1.3198708004461168 time is  1004.2074661254883\n",
      "142500 current avg lose is  1.321299134416865 time is  1021.8995621204376\n",
      "145000 current avg lose is  1.320986983209793 time is  1038.6785039901733\n",
      "147500 current avg lose is  1.3219430335184217 time is  1054.1812980175018\n",
      "150000 current avg lose is  1.3217860330345463 time is  1069.7205078601837\n",
      "152500 current avg lose is  1.322496251604019 time is  1084.9681990146637\n",
      "155000 current avg lose is  1.322955055061476 time is  1100.6275489330292\n",
      "157500 current avg lose is  1.3227956603052677 time is  1115.9846510887146\n",
      "160000 current avg lose is  1.3229543010432343 time is  1131.2285211086273\n"
     ]
    }
   ],
   "source": [
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "ii=0\n",
    "correct = 0\n",
    "for x_i,y_i in main_data:\n",
    "    line_tensor = mymodel_reviewtensor(x_i)\n",
    "    y_i = torch.tensor([y_i],dtype =torch.long)\n",
    "    output, loss = train(y_i, line_tensor)\n",
    "    current_loss += loss\n",
    "    ii += 1\n",
    "    if ii %2500 == 0 :\n",
    "        print(ii,\"current avg lose is \",current_loss/ii,'time is ',time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "80a70520",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_label = 2\n",
    "confusion = torch.zeros(n_label, n_label)\n",
    "\n",
    "def evaluate(line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "        \n",
    "      \n",
    "\n",
    "    return output\n",
    "\n",
    "for x1,y1 in test_data:\n",
    "    \n",
    "    label_tensor = torch.tensor([y1],dtype =torch.long)\n",
    "    review_tensor = mymodel_reviewtensor(x1)\n",
    "    output = evaluate(review_tensor)\n",
    "    \n",
    "    label_pred = output_predict_y(output)\n",
    "    confusion[y1][label_pred] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "db7c994b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_my_model_binary = confusion.trace()/confusion.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "b027989b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for my model with RNN binary is  tensor(0.8205)\n"
     ]
    }
   ],
   "source": [
    "print(\"the accuracy for my model with RNN binary is \",accuracy_my_model_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b132188",
   "metadata": {},
   "source": [
    "## binary RNN with google 300 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "6d5bbefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "opm = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "d267b79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review2tensor(x):\n",
    "    max_review = 50\n",
    "    ### create a matrix \n",
    "    review = torch.zeros(max_review,1,300)\n",
    "    pad = torch.zeros(300)\n",
    "    index = 0\n",
    "    for i in x:\n",
    "        if i in opm and index < 50:\n",
    "            vector = opm [i]\n",
    "            review[index][0] = torch.from_numpy(vector)\n",
    "            index += 1\n",
    "        else:\n",
    "            continue\n",
    "    if index < 50:\n",
    "        review[index][0] = pad\n",
    "    review = torch.nan_to_num(review)\n",
    "    return review \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "9b5622d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review2tensor('the').size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "a2654830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review2tensor(binary['review_body'].iloc[49])[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "f62d3bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(rnn.parameters(),0.5)\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "38683160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X = binary['review_body']\n",
    "Y = binary['y']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state = 19, test_size = 0.2)\n",
    "\n",
    "x_train = np.array(x_train.tolist())\n",
    "\n",
    "y_train = np.array(y_train.tolist())\n",
    "\n",
    "x_train = np.nan_to_num(x_train)\n",
    "\n",
    "x_test = np.array(x_test.tolist())\n",
    "\n",
    "y_test = np.array(y_test.tolist())\n",
    "\n",
    "x_test = np.nan_to_num(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "0e784e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "98ada69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "rnn = RNN(300, 50, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "40e929fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = review2tensor(binary['review_body'].iloc[4])\n",
    "hidden = torch.zeros(1, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "3ddfddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, next_hidden = rnn(input[0], hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "4c9becc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_predict_y(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    y = top_i[0].item()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "a59621f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yfromdata(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "c4c2783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "eefd1b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(rnn.parameters(),0.5)\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "862e2bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "5a875593",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data = list(zip(x_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8786aa40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "37f49512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 current avg lose is  0.6946149057388306 time is  22.118892192840576\n",
      "5000 current avg lose is  0.6957750918149949 time is  39.81695795059204\n",
      "7500 current avg lose is  0.6966212011496226 time is  56.48860502243042\n",
      "10000 current avg lose is  0.69695377741158 time is  73.53538584709167\n",
      "12500 current avg lose is  0.7033569383120537 time is  93.37862300872803\n",
      "15000 current avg lose is  0.7123965916832288 time is  114.35654783248901\n",
      "17500 current avg lose is  0.72068262171575 time is  131.81934809684753\n",
      "20000 current avg lose is  0.7233350892782211 time is  148.8105869293213\n",
      "22500 current avg lose is  0.7266608888400925 time is  166.8097460269928\n",
      "25000 current avg lose is  0.7288242967271805 time is  184.70361185073853\n",
      "27500 current avg lose is  0.7300440458850427 time is  203.6105260848999\n",
      "30000 current avg lose is  0.731794490335385 time is  221.02341198921204\n",
      "32500 current avg lose is  0.7336278566360473 time is  238.65603375434875\n",
      "35000 current avg lose is  0.7356945490990366 time is  257.80336809158325\n",
      "37500 current avg lose is  0.7380929750283559 time is  274.521723985672\n",
      "40000 current avg lose is  0.7402321053527295 time is  296.0276849269867\n",
      "42500 current avg lose is  0.7424549901247025 time is  316.3202168941498\n",
      "45000 current avg lose is  0.7431025891827212 time is  340.14389514923096\n",
      "47500 current avg lose is  0.7456497730669223 time is  363.267657995224\n",
      "50000 current avg lose is  0.7482634851193428 time is  385.98622012138367\n",
      "52500 current avg lose is  0.749999521695716 time is  403.04541087150574\n",
      "55000 current avg lose is  0.752858642465418 time is  423.7475640773773\n",
      "57500 current avg lose is  0.7547070155126893 time is  442.65362906455994\n",
      "60000 current avg lose is  0.7624538702889035 time is  458.9637677669525\n",
      "62500 current avg lose is  0.8110369722896015 time is  475.06502199172974\n",
      "65000 current avg lose is  0.8548702853062324 time is  491.0245599746704\n",
      "67500 current avg lose is  0.89210491614323 time is  506.9926769733429\n",
      "70000 current avg lose is  0.930156318563192 time is  524.7869830131531\n",
      "72500 current avg lose is  0.9642878387305945 time is  541.1039109230042\n",
      "75000 current avg lose is  0.9971683377809163 time is  557.5870459079742\n",
      "77500 current avg lose is  1.0274707935839082 time is  573.6545059680939\n",
      "80000 current avg lose is  1.0546235030927384 time is  590.199334859848\n",
      "82500 current avg lose is  1.0773018713259848 time is  605.8982841968536\n",
      "85000 current avg lose is  1.0952199241803813 time is  624.7830460071564\n",
      "87500 current avg lose is  1.115322752236526 time is  641.2154159545898\n",
      "90000 current avg lose is  1.1270416864774726 time is  657.0917508602142\n",
      "92500 current avg lose is  1.1424993010932045 time is  676.2039058208466\n",
      "95000 current avg lose is  1.1557575995815697 time is  706.4415879249573\n",
      "97500 current avg lose is  1.1680131034833308 time is  732.3915359973907\n",
      "100000 current avg lose is  1.179174193751666 time is  753.312472820282\n",
      "102500 current avg lose is  1.188754206022267 time is  775.7434687614441\n",
      "105000 current avg lose is  1.1963509835230297 time is  804.4075999259949\n",
      "107500 current avg lose is  1.2049114987721077 time is  829.9771678447723\n",
      "110000 current avg lose is  1.2120652044772853 time is  853.4426510334015\n",
      "112500 current avg lose is  1.2200559295490918 time is  870.8683519363403\n",
      "115000 current avg lose is  1.2276067807577682 time is  891.5767140388489\n",
      "117500 current avg lose is  1.2346387695620553 time is  915.5201261043549\n",
      "120000 current avg lose is  1.2397771903260764 time is  939.691565990448\n",
      "122500 current avg lose is  1.2444561228367543 time is  958.2355720996857\n",
      "125000 current avg lose is  1.2485308202114516 time is  974.4230170249939\n",
      "127500 current avg lose is  1.2530430875246032 time is  990.3677740097046\n",
      "130000 current avg lose is  1.256757725178107 time is  1006.3707869052887\n",
      "132500 current avg lose is  1.2629742058953475 time is  1022.3463580608368\n",
      "135000 current avg lose is  1.2670394796028586 time is  1039.089103937149\n",
      "137500 current avg lose is  1.2707857760186643 time is  1059.609256029129\n",
      "140000 current avg lose is  1.2759001230743308 time is  1081.2359149456024\n",
      "142500 current avg lose is  1.2796641287027934 time is  1097.3742470741272\n",
      "145000 current avg lose is  1.2841903255463907 time is  1113.232172012329\n",
      "147500 current avg lose is  1.288557664455274 time is  1130.874839067459\n",
      "150000 current avg lose is  1.292357830940174 time is  1147.2331640720367\n",
      "152500 current avg lose is  1.295247374925193 time is  1163.402438879013\n",
      "155000 current avg lose is  1.298593524060938 time is  1179.3178389072418\n",
      "157500 current avg lose is  1.3025559112659642 time is  1195.6890890598297\n",
      "160000 current avg lose is  1.3047873583760832 time is  1212.0009269714355\n"
     ]
    }
   ],
   "source": [
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "ii=0\n",
    "correct = 0\n",
    "for x_i,y_i in main_data:\n",
    "    line_tensor = review2tensor(x_i)\n",
    "    y_i = torch.tensor([y_i],dtype =torch.long)\n",
    "    output, loss = train(y_i, line_tensor)\n",
    "    current_loss += loss\n",
    "    ii += 1\n",
    "    predict_y = yfromdata(output)\n",
    "    if y_i == predict_y:\n",
    "        correct +=1\n",
    "    if ii %2500 == 0 :\n",
    "        print(ii,\"current avg lose is \",current_loss/ii,'time is ',time.time() - start)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "650b17ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005920564290136099"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "59e63457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208765.9773401733"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "cd1163d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = list(zip(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "4b525e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_label = 2\n",
    "confusion = torch.zeros(n_label, n_label)\n",
    "\n",
    "def evaluate(line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "        \n",
    "      \n",
    "\n",
    "    return output\n",
    "\n",
    "for x1,y1 in test_data:\n",
    "    \n",
    "    label_tensor = torch.tensor([y1],dtype =torch.long)\n",
    "    review_tensor = review2tensor(x1)\n",
    "    output = evaluate(review_tensor)\n",
    "    \n",
    "    label_pred = output_predict_y(output)\n",
    "    confusion[y1][label_pred] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "d0df4607",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_googel_300_binary = confusion.trace()/confusion.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "735f12a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for googel 300 model with RNN binary is  tensor(0.7890)\n"
     ]
    }
   ],
   "source": [
    "print(\"the accuracy for googel 300 model with RNN binary is \",accuracy_googel_300_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ff128a",
   "metadata": {},
   "source": [
    "## trinary with my model RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "136c5be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mymodel_reviewtensor(x):\n",
    "    max_review = 50\n",
    "    ### create a matrix \n",
    "    review = torch.zeros(max_review,1,300)\n",
    "    pad = torch.zeros(300)\n",
    "    index = 0\n",
    "    for i in x:\n",
    "        if i in model_1.wv and index < 50:\n",
    "            vector = model_1.wv [i]\n",
    "            review[index][0] = torch.from_numpy(vector)\n",
    "            index += 1\n",
    "        else:\n",
    "            continue\n",
    "    if index < 50:\n",
    "        review[index][0] = pad\n",
    "    review = torch.nan_to_num(review)\n",
    "    return review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "5bb5396c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X = new_test['review_body']\n",
    "Y = new_test['y']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state = 19, test_size = 0.2)\n",
    "\n",
    "x_train = np.array(x_train.tolist())\n",
    "\n",
    "y_train = np.array(y_train.tolist())\n",
    "\n",
    "x_train = np.nan_to_num(x_train)\n",
    "\n",
    "x_test = np.array(x_test.tolist())\n",
    "\n",
    "y_test = np.array(y_test.tolist())\n",
    "\n",
    "x_test = np.nan_to_num(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "b03b1977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "rnn = RNN(300, 50, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "2b9dcda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = review2tensor(binary['review_body'].iloc[4])\n",
    "hidden = torch.zeros(1, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "ff5813ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "b4a4a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(rnn.parameters(),0.5)\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "0b23a3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data = list(zip(x_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "f50d06a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 current avg lose is  1.058883339238167 time is  32.93949484825134\n",
      "5000 current avg lose is  1.0648978867650032 time is  69.53135681152344\n",
      "7500 current avg lose is  1.0671727425018946 time is  101.4669828414917\n",
      "10000 current avg lose is  1.070502868539095 time is  137.4072709083557\n",
      "12500 current avg lose is  1.0674601861286164 time is  171.49279308319092\n",
      "15000 current avg lose is  1.0676490481615066 time is  212.4065079689026\n",
      "17500 current avg lose is  1.0673559479747499 time is  238.32458591461182\n",
      "20000 current avg lose is  1.0682490519195795 time is  265.84134697914124\n",
      "22500 current avg lose is  1.0685526640786065 time is  296.73723793029785\n",
      "25000 current avg lose is  1.067924862034321 time is  340.1728708744049\n",
      "27500 current avg lose is  1.0683107580510052 time is  377.68309903144836\n",
      "30000 current avg lose is  1.0681639694203933 time is  402.9458739757538\n",
      "32500 current avg lose is  1.06844231048639 time is  435.8811500072479\n",
      "35000 current avg lose is  1.0687974279531411 time is  474.25870418548584\n",
      "37500 current avg lose is  1.0680510608522098 time is  502.68456196784973\n",
      "40000 current avg lose is  1.0679307772897184 time is  528.8641378879547\n",
      "42500 current avg lose is  1.067544614909677 time is  549.3223950862885\n",
      "45000 current avg lose is  1.0671093422227436 time is  574.4050459861755\n",
      "47500 current avg lose is  1.0668882419184635 time is  593.3297369480133\n",
      "50000 current avg lose is  1.0673958583831786 time is  611.2758169174194\n",
      "52500 current avg lose is  1.0671651781110536 time is  629.3913021087646\n",
      "55000 current avg lose is  1.0674287337704138 time is  647.611419916153\n",
      "57500 current avg lose is  1.0673986786463987 time is  666.7348101139069\n",
      "60000 current avg lose is  1.0674292934546867 time is  686.1097941398621\n",
      "62500 current avg lose is  1.0676897953548432 time is  704.9740071296692\n",
      "65000 current avg lose is  1.0675282738011618 time is  723.0551240444183\n",
      "67500 current avg lose is  1.0679157680847027 time is  742.5105309486389\n",
      "70000 current avg lose is  1.0681421054674047 time is  774.5150830745697\n",
      "72500 current avg lose is  1.0681621937542125 time is  795.1100499629974\n",
      "75000 current avg lose is  1.068906489068667 time is  816.1326999664307\n",
      "77500 current avg lose is  1.0688752756503321 time is  840.0066690444946\n",
      "80000 current avg lose is  1.0687999247074127 time is  859.6933438777924\n",
      "82500 current avg lose is  1.0688307638681296 time is  879.3936541080475\n",
      "85000 current avg lose is  1.0688828008555313 time is  898.4715600013733\n",
      "87500 current avg lose is  1.0692061602795124 time is  916.1872630119324\n",
      "90000 current avg lose is  1.069229385611746 time is  932.1801540851593\n",
      "92500 current avg lose is  1.0693909089492786 time is  955.1374840736389\n",
      "95000 current avg lose is  1.0696662023854884 time is  982.492262840271\n",
      "97500 current avg lose is  1.0700385442831577 time is  1005.0275959968567\n",
      "100000 current avg lose is  1.0703866867625713 time is  1031.2568221092224\n",
      "102500 current avg lose is  1.070607228088815 time is  1053.1853449344635\n",
      "105000 current avg lose is  1.0708351706147194 time is  1071.345862865448\n",
      "107500 current avg lose is  1.0711136703724085 time is  1090.9855217933655\n",
      "110000 current avg lose is  1.0715125057807022 time is  1110.938469171524\n",
      "112500 current avg lose is  1.071382676229477 time is  1132.2656807899475\n",
      "115000 current avg lose is  1.07182084618457 time is  1149.6435360908508\n",
      "117500 current avg lose is  1.0721704680085182 time is  1168.021145105362\n",
      "120000 current avg lose is  1.0723628629346689 time is  1185.6967658996582\n",
      "122500 current avg lose is  1.072538261650533 time is  1201.6084110736847\n",
      "125000 current avg lose is  1.0732937121067048 time is  1217.6509392261505\n",
      "127500 current avg lose is  1.0736993805029813 time is  1233.139272928238\n",
      "130000 current avg lose is  1.0738127784848213 time is  1248.815110206604\n",
      "132500 current avg lose is  1.0744151264868818 time is  1264.4576029777527\n",
      "135000 current avg lose is  1.0749497592378545 time is  1280.150857925415\n",
      "137500 current avg lose is  1.0754431096032533 time is  1296.505954027176\n",
      "140000 current avg lose is  1.0762516807617886 time is  1312.7235021591187\n",
      "142500 current avg lose is  1.0767683569736648 time is  1328.55117893219\n",
      "145000 current avg lose is  1.0778381568373276 time is  1344.4150059223175\n",
      "147500 current avg lose is  1.0789071250371003 time is  1360.0745420455933\n",
      "150000 current avg lose is  1.080071480565369 time is  1375.9857861995697\n",
      "152500 current avg lose is  1.0812610080664276 time is  1393.290606021881\n",
      "155000 current avg lose is  1.0820124134346363 time is  1413.3638708591461\n",
      "157500 current avg lose is  1.08362149730812 time is  1430.3814189434052\n",
      "160000 current avg lose is  1.0879190420290192 time is  1445.9106590747833\n",
      "162500 current avg lose is  1.095478659599033 time is  1461.8841269016266\n",
      "165000 current avg lose is  1.1105091630237296 time is  1478.5193021297455\n",
      "167500 current avg lose is  1.1374214921888 time is  1494.359342098236\n",
      "170000 current avg lose is  1.1600518989381232 time is  1510.375832080841\n",
      "172500 current avg lose is  1.1817067828752488 time is  1526.18967294693\n",
      "175000 current avg lose is  1.20119316435376 time is  1541.6961488723755\n",
      "177500 current avg lose is  1.2194974404940115 time is  1557.3090360164642\n",
      "180000 current avg lose is  1.2366499543420968 time is  1572.9594340324402\n",
      "182500 current avg lose is  1.2567743561924472 time is  1588.428687095642\n",
      "185000 current avg lose is  1.2738383172162429 time is  1603.9515030384064\n",
      "187500 current avg lose is  1.2889522607763801 time is  1619.5939869880676\n",
      "190000 current avg lose is  1.3030570407156976 time is  1635.0615639686584\n",
      "192500 current avg lose is  1.3172570710113864 time is  1650.428759098053\n",
      "195000 current avg lose is  1.3314729056965577 time is  1665.848848104477\n",
      "197500 current avg lose is  1.3469677912197182 time is  1681.2876300811768\n",
      "200000 current avg lose is  1.3612941409675718 time is  1696.6154670715332\n"
     ]
    }
   ],
   "source": [
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "ii=0\n",
    "correct = 0\n",
    "for x_i,y_i in main_data:\n",
    "    line_tensor = review2tensor(x_i)\n",
    "    y_i = torch.tensor([y_i],dtype =torch.long)\n",
    "    output, loss = train(y_i, line_tensor)\n",
    "    current_loss += loss\n",
    "    ii += 1\n",
    "    if ii %2500 == 0 :\n",
    "        print(ii,\"current avg lose is \",current_loss/ii,'time is ',time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "c6b32c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = list(zip(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "688143fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_label = 3\n",
    "confusion = torch.zeros(n_label, n_label)\n",
    "\n",
    "def evaluate(line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "        \n",
    "      \n",
    "\n",
    "    return output\n",
    "\n",
    "for x1,y1 in test_data:\n",
    "    \n",
    "    label_tensor = torch.tensor([y1],dtype =torch.long)\n",
    "    review_tensor = review2tensor(x1)\n",
    "    output = evaluate(review_tensor)\n",
    "    \n",
    "    label_pred = output_predict_y(output)\n",
    "    confusion[y1][label_pred] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "7eca181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_my_model_ternary = confusion.trace()/confusion.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "4b929a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6221)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_my_model_ternary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "190ab7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for my model with RNN binary is  tensor(0.6221)\n"
     ]
    }
   ],
   "source": [
    "print(\"the accuracy for my model with RNN binary is \",accuracy_my_model_ternary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9661cf77",
   "metadata": {},
   "source": [
    "## trinary with google 300 model RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "77864016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opm_reviewtensor(x):\n",
    "    max_review = 50\n",
    "    ### create a matrix \n",
    "    review = torch.zeros(max_review,1,300)\n",
    "    pad = torch.zeros(300)\n",
    "    index = 0\n",
    "    for i in x:\n",
    "        if i in opm and index < 50:\n",
    "            vector = opm [i]\n",
    "            review[index][0] = torch.from_numpy(vector)\n",
    "            index += 1\n",
    "        else:\n",
    "            continue\n",
    "    if index < 50:\n",
    "        review[index][0] = pad\n",
    "    review = torch.nan_to_num(review)\n",
    "    return review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "08c97f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "rnn = RNN(300, 50, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "8d9c14ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "6d376649",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(rnn.parameters(),0.5)\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "b618cb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data = list(zip(x_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "c0882cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 current avg lose is  1.0594981743097305 time is  23.336733102798462\n",
      "5000 current avg lose is  1.064748005270958 time is  46.70505118370056\n",
      "7500 current avg lose is  1.0665473565498989 time is  63.84702515602112\n",
      "10000 current avg lose is  1.069900180619955 time is  81.04995703697205\n",
      "12500 current avg lose is  1.0668171494627 time is  98.48082613945007\n",
      "15000 current avg lose is  1.0672467289964358 time is  116.14186596870422\n",
      "17500 current avg lose is  1.066933060513224 time is  133.63144707679749\n",
      "20000 current avg lose is  1.067680196505785 time is  150.6814329624176\n",
      "22500 current avg lose is  1.0679751199086507 time is  167.5362868309021\n",
      "25000 current avg lose is  1.067366377673149 time is  184.75301909446716\n",
      "27500 current avg lose is  1.067827658234943 time is  201.7656970024109\n",
      "30000 current avg lose is  1.0678597442567348 time is  218.99542498588562\n",
      "32500 current avg lose is  1.0681703750610352 time is  236.04149508476257\n",
      "35000 current avg lose is  1.0684610444409506 time is  253.11390805244446\n",
      "37500 current avg lose is  1.0677561033082008 time is  273.18121314048767\n",
      "40000 current avg lose is  1.0677225017450749 time is  290.8098340034485\n",
      "42500 current avg lose is  1.0673887481205604 time is  307.96116185188293\n",
      "45000 current avg lose is  1.0670583236687714 time is  325.34737396240234\n",
      "47500 current avg lose is  1.0668896239531667 time is  342.1371760368347\n",
      "50000 current avg lose is  1.0674965506768226 time is  359.2518639564514\n",
      "52500 current avg lose is  1.0672683788594746 time is  380.49341201782227\n",
      "55000 current avg lose is  1.0675473557634787 time is  402.39842104911804\n",
      "57500 current avg lose is  1.06754001811069 time is  419.7279779911041\n",
      "60000 current avg lose is  1.0675953443129858 time is  436.927148103714\n",
      "62500 current avg lose is  1.0678798127880096 time is  454.3286690711975\n",
      "65000 current avg lose is  1.0677606553531611 time is  471.0404679775238\n",
      "67500 current avg lose is  1.0681976647487392 time is  487.91938495635986\n",
      "70000 current avg lose is  1.068524079589333 time is  504.5579659938812\n",
      "72500 current avg lose is  1.0685647462355679 time is  521.088385105133\n",
      "75000 current avg lose is  1.0694102822200457 time is  541.2910070419312\n",
      "77500 current avg lose is  1.0693500262260438 time is  558.7195630073547\n",
      "80000 current avg lose is  1.0693192850824444 time is  575.1268022060394\n",
      "82500 current avg lose is  1.0693833013919267 time is  594.3808679580688\n",
      "85000 current avg lose is  1.0694949775364468 time is  611.840841293335\n",
      "87500 current avg lose is  1.0698560204439505 time is  628.5954940319061\n",
      "90000 current avg lose is  1.069898481336236 time is  645.9558482170105\n",
      "92500 current avg lose is  1.0700374788427676 time is  664.6808271408081\n",
      "95000 current avg lose is  1.070354220152372 time is  682.0515701770782\n",
      "97500 current avg lose is  1.0707094923407603 time is  698.7866830825806\n",
      "100000 current avg lose is  1.0710481071619689 time is  716.7230660915375\n",
      "102500 current avg lose is  1.0712487298258921 time is  734.2472088336945\n",
      "105000 current avg lose is  1.071468197950437 time is  751.8574929237366\n",
      "107500 current avg lose is  1.0717564312385959 time is  768.8746490478516\n",
      "110000 current avg lose is  1.0721288732883605 time is  785.8098080158234\n",
      "112500 current avg lose is  1.0719723090370497 time is  804.3105058670044\n",
      "115000 current avg lose is  1.0723668803686681 time is  823.2005460262299\n",
      "117500 current avg lose is  1.072707923695128 time is  844.9211530685425\n",
      "120000 current avg lose is  1.072853413661197 time is  861.6371009349823\n",
      "122500 current avg lose is  1.0729630950057993 time is  877.2556829452515\n",
      "125000 current avg lose is  1.073640887413144 time is  893.0236148834229\n",
      "127500 current avg lose is  1.0740095382137627 time is  908.8859710693359\n",
      "130000 current avg lose is  1.0740493706464767 time is  924.9225609302521\n",
      "132500 current avg lose is  1.0745534437333637 time is  940.3724489212036\n",
      "135000 current avg lose is  1.0749703991068733 time is  955.9236941337585\n",
      "137500 current avg lose is  1.0753464512024142 time is  971.6035671234131\n",
      "140000 current avg lose is  1.0759549313152474 time is  987.861456155777\n",
      "142500 current avg lose is  1.0762306944811553 time is  1003.5206110477448\n",
      "145000 current avg lose is  1.0768760815870144 time is  1019.2891991138458\n",
      "147500 current avg lose is  1.0775488885355198 time is  1034.8012599945068\n",
      "150000 current avg lose is  1.07824242566665 time is  1049.9733700752258\n",
      "152500 current avg lose is  1.0788664567546766 time is  1065.3668999671936\n",
      "155000 current avg lose is  1.0792265710586502 time is  1080.6259188652039\n",
      "157500 current avg lose is  1.0798717930496684 time is  1096.1389381885529\n",
      "160000 current avg lose is  1.0810140441008378 time is  1111.9292871952057\n",
      "162500 current avg lose is  1.0829105518394708 time is  1127.6568629741669\n",
      "165000 current avg lose is  1.086564545000401 time is  1142.9311730861664\n",
      "167500 current avg lose is  1.092441711496133 time is  1158.6104590892792\n",
      "170000 current avg lose is  1.1002550451258148 time is  1173.8612530231476\n",
      "172500 current avg lose is  1.1275077948343504 time is  1195.6613211631775\n",
      "175000 current avg lose is  1.1503944251237956 time is  1221.9788782596588\n",
      "177500 current avg lose is  1.1701684366750869 time is  1239.8266541957855\n",
      "180000 current avg lose is  1.1884959107016662 time is  1255.7306549549103\n",
      "182500 current avg lose is  1.2083724974900445 time is  1271.3427398204803\n",
      "185000 current avg lose is  1.2261250610874894 time is  1299.1251759529114\n",
      "187500 current avg lose is  1.2426133169996108 time is  1326.855817079544\n",
      "190000 current avg lose is  1.258071125571814 time is  1345.0316240787506\n",
      "192500 current avg lose is  1.2726532343440726 time is  1362.7227640151978\n",
      "195000 current avg lose is  1.2865889542886226 time is  1380.7086811065674\n",
      "197500 current avg lose is  1.3023360601757314 time is  1398.571238040924\n",
      "200000 current avg lose is  1.3172428433795291 time is  1416.3848860263824\n"
     ]
    }
   ],
   "source": [
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "ii=0\n",
    "correct = 0\n",
    "for x_i,y_i in main_data:\n",
    "    line_tensor = opm_reviewtensor(x_i)\n",
    "    y_i = torch.tensor([y_i],dtype =torch.long)\n",
    "    output, loss = train(y_i, line_tensor)\n",
    "    current_loss += loss\n",
    "    ii += 1\n",
    "    if ii %2500 == 0 :\n",
    "        print(ii,\"current avg lose is \",current_loss/ii,'time is ',time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "415c5260",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_opm_ternary = confusion.trace()/confusion.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "2ce4a7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy for google model with RNN binary is  tensor(0.6221)\n"
     ]
    }
   ],
   "source": [
    "print(\"the accuracy for google model with RNN binary is \",accuracy_opm_ternary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85479188",
   "metadata": {},
   "source": [
    "#RNN\n",
    "\n",
    "RNN : Binary Case with google model | The Test Accuracy is 0.7890\n",
    "\n",
    "RNN : Ternary case with googel model | the Test Accuracy is 0.6221\n",
    "\n",
    "RNN: Binary case with my own model | Test Accuracy is 0.8205\n",
    "\n",
    "RNN: Ternary case with my own model | Test Accuracy is 0.6223\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffe32f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
