{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c54f0dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "import gzip\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e5d692",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2bb9111",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efe4d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train', names=['s_idx','word','tag'], delim_whitespace=True, \\\n",
    "                       keep_default_na=False, engine='python')\n",
    "dev_df = pd.read_csv('./data/dev', names=['s_idx','word','tag'], delim_whitespace=True, \\\n",
    "                       keep_default_na=False, engine='python')\n",
    "test_df = pd.read_csv('./data/test', names=['s_idx','word'], delim_whitespace=True, \\\n",
    "                       keep_default_na=False, engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efabb29",
   "metadata": {},
   "source": [
    "# Prepare vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b617b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df, word_vocab, tags_vocab):\n",
    "    x = []\n",
    "    y = []\n",
    "    sentence = []\n",
    "    tags = []\n",
    "    for idx, word in enumerate(df['word']):\n",
    "        if df.iloc[idx].s_idx==1:\n",
    "            if idx != 0:\n",
    "                x.append(torch.tensor(sentence, dtype=torch.long))\n",
    "                y.append(torch.tensor(tags, dtype=torch.long))\n",
    "                sentence = []\n",
    "                tags = []\n",
    "        sentence.append(word_vocab[word] if word in word_vocab else word_vocab['<unk>'])\n",
    "        tags.append(tags_vocab[df.iloc[idx].tag])\n",
    "        if idx == len(df) -1:\n",
    "            x.append(torch.tensor(sentence, dtype=torch.long))\n",
    "            y.append(torch.tensor(tags, dtype=torch.long))\n",
    "    return x, y\n",
    "\n",
    "\n",
    "\n",
    "def create_test_dataset(df, word_vocab):\n",
    "    x = []\n",
    "    sentence = []\n",
    "    for idx, word in enumerate(df['word']):\n",
    "        if df.iloc[idx].s_idx==1:\n",
    "            if idx != 0:\n",
    "                x.append(torch.tensor(sentence, dtype=torch.long))\n",
    "                sentence = [] \n",
    "        sentence.append(word_vocab[word] if word in word_vocab else word_vocab['<unk>'])\n",
    "        if idx == len(df) -1:\n",
    "            x.append(torch.tensor(sentence, dtype=torch.long))\n",
    "    return x\n",
    "\n",
    "def decode_ner(data, tags_vocab):\n",
    "    decode_list = []\n",
    "    for i in data:\n",
    "        for j in i:\n",
    "            decode_list.append(tags_vocab[int(j)])\n",
    "    return decode_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb5c57f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab = {}\n",
    "tags_vocab = {}\n",
    "decode_tags_vocab = {}\n",
    "for idx, word in enumerate(train_df['word'].value_counts().keys()):\n",
    "    word_vocab[word] = idx\n",
    "for idx, tag in enumerate(train_df['tag'].value_counts().keys()):\n",
    "    tags_vocab[tag] = idx\n",
    "    decode_tags_vocab[idx] = tag\n",
    "word_vocab['<unk>'] = len(word_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc71be26",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_word_embeddings = {}\n",
    "tmp_embed = nn.Embedding(len(word_vocab), 100)\n",
    "for i, word in enumerate(word_vocab.keys()):\n",
    "    random_word_embeddings[word] = tmp_embed(torch.LongTensor([i]))[0].tolist()\n",
    "glove_vocab = {}\n",
    "with gzip.open('glove.6B.100d.gz','r') as f:        \n",
    "    for line in f:   \n",
    "        tmp = line.split()\n",
    "        word = tmp[0].decode('utf-8')\n",
    "        v = [float(x.decode('utf-8')) for x in tmp[1:]]\n",
    "        glove_vocab[word] = v\n",
    "not_list_word = []\n",
    "for idx, word in enumerate(random_word_embeddings.keys()):\n",
    "    if word not in glove_vocab:\n",
    "        not_list_word.append(word)\n",
    "for word in not_list_word:\n",
    "    if word.lower() not in glove_vocab:\n",
    "        glove_vocab[word] = random_word_embeddings[word]\n",
    "    else:\n",
    "        glove_vocab[word] = glove_vocab[word.lower()]\n",
    "glove_vocab_list = {}\n",
    "for idx, word in enumerate(glove_vocab.keys()):\n",
    "    glove_vocab_list[word] = idx\n",
    "glove_embedding = [glove_vocab[key] for key in glove_vocab_list.keys()]\n",
    "glove_embedding = torch.FloatTensor(glove_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa3fc6c",
   "metadata": {},
   "source": [
    "# Task 1: Simple Bidirectional LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8441d59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, linear_dim, vocab_size, tagset_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.linear_dim = linear_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim//2, dropout=0.33, num_layers=1, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.fc = nn.Linear(hidden_dim, linear_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.hidden2tag = nn.Linear(linear_dim, tagset_size)\n",
    "        self.h, self.c = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return torch.randn(2, 1, self.hidden_dim // 2), torch.randn(2, 1, self.hidden_dim // 2)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        self.h, self.c = self.init_hidden()\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, (self.h, self.c) = self.lstm(embeds.view(len(sentence), 1, -1), (self.h.to(device), self.c.to(device)))\n",
    "        drop_out = self.dropout(lstm_out)\n",
    "        fc_out = self.fc(drop_out)\n",
    "        elu_out = self.elu(fc_out)\n",
    "        tag_space = self.hidden2tag(elu_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e50c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up dataloader\n",
    "train_x, train_y = create_dataset(train_df, word_vocab, tags_vocab)\n",
    "train_loader = [(train_x[i],train_y[i]) for i in range(len(train_x))]\n",
    "dev_x, dev_y = create_dataset(dev_df, word_vocab, tags_vocab)\n",
    "dev_loader = [(dev_x[i],dev_y[i]) for i in range(len(dev_x))]\n",
    "test_x = create_test_dataset(test_df, word_vocab)\n",
    "test_loader = [test_x[i] for i in range(len(test_x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bc4aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model\n",
    "model = LSTM(100, 256, 128, len(word_vocab), len(tags_vocab)).to(device)\n",
    "loss_function = nn.NLLLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d05852a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███▉                                                                           | 10/200 [06:36<2:05:02, 39.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9:  tensor(0.0929, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▉                                                                       | 20/200 [13:10<1:58:19, 39.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19:  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|███████████▊                                                                   | 30/200 [19:44<1:51:43, 39.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29:  tensor(0.0246, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████▊                                                               | 40/200 [26:18<1:44:53, 39.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39:  tensor(0.0233, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████████████▊                                                           | 50/200 [32:52<1:38:24, 39.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49:  tensor(0.0155, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████▋                                                       | 60/200 [39:26<1:32:07, 39.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 59:  tensor(0.0118, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███████████████████████████▋                                                   | 70/200 [46:01<1:25:24, 39.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 69:  tensor(0.0128, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████▌                                               | 80/200 [52:34<1:18:40, 39.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 79:  tensor(0.0099, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|███████████████████████████████████▌                                           | 90/200 [59:08<1:12:12, 39.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 89:  tensor(0.0068, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████                                      | 100/200 [1:05:41<1:05:36, 39.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 99:  tensor(0.0088, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|██████████████████████████████████████████▉                                   | 110/200 [1:12:15<59:01, 39.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 109:  tensor(0.0069, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████▊                               | 120/200 [1:18:48<52:31, 39.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 119:  tensor(0.0065, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████████████████████████████████████████████████▋                           | 130/200 [1:25:22<45:54, 39.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 129:  tensor(0.0056, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████▌                       | 140/200 [1:31:55<39:21, 39.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 139:  tensor(0.0052, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|██████████████████████████████████████████████████████████▌                   | 150/200 [1:38:29<32:47, 39.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 149:  tensor(0.0055, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████▍               | 160/200 [1:45:03<26:15, 39.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 159:  tensor(0.0047, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|██████████████████████████████████████████████████████████████████▎           | 170/200 [1:51:36<19:40, 39.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 169:  tensor(0.0042, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████████████████████████▏       | 180/200 [1:58:10<13:06, 39.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 179:  tensor(0.0031, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|██████████████████████████████████████████████████████████████████████████    | 190/200 [2:04:44<06:33, 39.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 189:  tensor(0.0030, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 200/200 [2:11:18<00:00, 39.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 199:  tensor(0.0031, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model.train()\n",
    "cnt = 0\n",
    "for epoch in tqdm(range(200)):\n",
    "    epoch_loss = 0.0\n",
    "    cnt += 1\n",
    "    for sentence, tags in train_loader:\n",
    "        model.zero_grad()\n",
    "        tag_scores = model(sentence.to(device))\n",
    "        loss = loss_function(tag_scores, tags.to(device))\n",
    "        epoch_loss+=loss\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "    if cnt == 10:\n",
    "        print(f\"epoch {epoch}: \", epoch_loss/len(train_loader))\n",
    "        cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9832f51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "model.eval()\n",
    "dev_predit =[]\n",
    "with torch.no_grad():\n",
    "    for sentence, tags in dev_loader:\n",
    "        tag_scores = model(sentence.to(device))\n",
    "        dev_predit.append(torch.argmax(tag_scores,dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d45e6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_decode_pred = decode_ner(dev_predit, decode_tags_vocab)\n",
    "dev_df['pred'] = dev_decode_pred\n",
    "dev_df.to_csv('dev1.out', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "635a04f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predeiction\n",
    "model.eval()\n",
    "test_predit =[]\n",
    "with torch.no_grad():\n",
    "    for sentence in test_loader:\n",
    "        tag_scores = model(sentence.to(device))\n",
    "        test_predit.append(torch.argmax(tag_scores,dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a8eca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_decode_pred = decode_ner(test_predit, decode_tags_vocab)\n",
    "test_df['pred'] = test_decode_pred\n",
    "test_df.to_csv('test1.out', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b9c392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'blstm1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60bdfc8",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e1e7e456",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_t2(nn.Module):\n",
    "\n",
    "    def __init__(self, glove_embedding, embedding_dim, hidden_dim, linear_dim, tagset_size):\n",
    "        super(LSTM_t2, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.linear_dim = linear_dim\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(glove_embedding, freeze=True)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim//2, dropout=0.33, num_layers=1, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.fc = nn.Linear(hidden_dim, linear_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.hidden2tag = nn.Linear(linear_dim, tagset_size)\n",
    "        self.h, self.c = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return torch.randn(2, 1, self.hidden_dim // 2), torch.randn(2, 1, self.hidden_dim // 2)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        self.h, self.c = self.init_hidden()\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, (self.h, self.c) = self.lstm(embeds.view(len(sentence), 1, -1), (self.h.to(device), self.c.to(device)))\n",
    "        drop_out = self.dropout(lstm_out)\n",
    "        fc_out = self.fc(drop_out)\n",
    "        elu_out = self.elu(fc_out)\n",
    "        tag_space = self.hidden2tag(elu_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01154826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up dataloader\n",
    "train_x_t2, train_y_t2 = create_dataset(train_df, glove_vocab_list, tags_vocab)\n",
    "train_loader_t2 = [(train_x_t2[i],train_y_t2[i]) for i in range(len(train_x_t2))]\n",
    "dev_x_t2, dev_y_t2 = create_dataset(dev_df, glove_vocab_list, tags_vocab)\n",
    "dev_loader_t2 = [(dev_x_t2[i],dev_y_t2[i]) for i in range(len(dev_x_t2))]\n",
    "test_x_t2 = create_test_dataset(test_df, glove_vocab_list)\n",
    "test_loader_t2 = [test_x_t2[i] for i in range(len(test_x_t2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f9a6e5af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set up model\n",
    "task2_model = LSTM_t2(glove_embedding, 100, 256, 128, len(tags_vocab)).to(device)\n",
    "task2_loss_function = nn.NLLLoss().to(device)\n",
    "task2_optimizer = torch.optim.SGD(task2_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274d102b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▉                                                                       | 10/100 [07:19<1:05:53, 43.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10:  tensor(0.1612, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▏                                                                | 20/100 [14:37<58:22, 43.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20:  tensor(0.1163, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████▎                                                        | 30/100 [21:40<48:39, 41.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30:  tensor(0.0906, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████▍                                                | 40/100 [28:33<41:17, 41.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40:  tensor(0.0792, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████████▌                                        | 50/100 [35:26<34:29, 41.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50:  tensor(0.0657, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████▌                                | 60/100 [42:19<27:33, 41.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 60:  tensor(0.0589, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|███████████████████████████████████████████████████                              | 63/100 [44:24<25:37, 41.56s/it]"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "task2_model.train()\n",
    "cnt = 0\n",
    "for epoch in tqdm(range(100)):\n",
    "    epoch_loss = 0.0\n",
    "    cnt += 1\n",
    "    for sentence, tags in train_loader_t2:\n",
    "        task2_model.zero_grad()\n",
    "        tag_scores = task2_model(sentence.to(device))\n",
    "        loss = task2_loss_function(tag_scores, tags.to(device))\n",
    "        epoch_loss+=loss\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(task2_model.parameters(), 5)\n",
    "        task2_optimizer.step()\n",
    "    if cnt == 10:\n",
    "        print(f\"epoch {epoch+1}: \", epoch_loss/len(train_loader_t2))\n",
    "        cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cb891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "task2_model.eval()\n",
    "dev_predit_t2 =[]\n",
    "with torch.no_grad():\n",
    "    for sentence, tags in dev_loader_t2:\n",
    "        tag_scores = task2_model(sentence.to(device))\n",
    "        dev_predit_t2.append(torch.argmax(tag_scores,dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f4fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_decode_pred_t2 = decode_ner(dev_predit_t2, decode_tags_vocab)\n",
    "dev_df_t2 = dev_df\n",
    "dev_df_t2['pred'] = dev_decode_pred_t2\n",
    "dev_df_t2.to_csv('dev2.out', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d875b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "task2_model.eval()\n",
    "test_predit_t2 =[]\n",
    "with torch.no_grad():\n",
    "    for sentence in test_loader_t2:\n",
    "        tag_scores = task2_model(sentence.to(device))\n",
    "        test_predit_t2.append(torch.argmax(tag_scores,dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1136e880",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_decode_pred_t2 = decode_ner(test_predit_t2, decode_tags_vocab)\n",
    "test_df_t2 = test_df\n",
    "test_df_t2['pred'] = test_decode_pred_t2\n",
    "test_df_t2.to_csv('test2.out', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092e7b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(task2_model, 'blstm2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6549d847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference:\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "# https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab7c734",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
