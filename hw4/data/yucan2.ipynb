{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ebab432",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import SGD\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import csv\n",
    "\n",
    "data_file = pd.read_csv('train', quoting=csv.QUOTE_NONE,sep=\" \", names= ['index', 'word', 'tag'])\n",
    "data_array = data_file.to_numpy()\n",
    "\n",
    "dev_file = pd.read_csv('dev', quoting=csv.QUOTE_NONE,sep=\" \", names= ['index', 'word', 'tag'])\n",
    "dev_array = dev_file.to_numpy()\n",
    "\n",
    "\n",
    "### concert each word into lower case and generate sentence tag list \n",
    "def converter(data):\n",
    "    train_word = []\n",
    "    train_tag = []\n",
    "    i = 0\n",
    "    for index, word,tag in data:\n",
    "        if index == 1:\n",
    "            temp1 = []\n",
    "            temp2 = []\n",
    "            temp1.append(str(word).lower()) ## convert the word into lower case \n",
    "            temp2.append(tag)\n",
    "            \n",
    "        else:\n",
    "            temp1.append(str(word).lower())\n",
    "            temp2.append(tag)\n",
    "        if ( (i+1 < len(data)) and data[i+1][0] == 1 ) or (i == len(data)-1):\n",
    "            train_word.append(temp1)\n",
    "            train_tag.append(temp2)\n",
    "        i += 1\n",
    "    return train_word, train_tag\n",
    "    \n",
    "\n",
    "train_word, train_tag = converter(data_array)\n",
    "all_data_list = list(zip(train_word, train_tag ))\n",
    "\n",
    "\n",
    "dev_word, dev_tag = converter(dev_array)\n",
    "\n",
    "\n",
    "### add words in dev to train vocab since we have glove as our embedding \n",
    "train_word.extend(dev_word)\n",
    "\n",
    "\n",
    "## create the word vocab\n",
    "w_vocab =  ['<unk>']+['<pad>'] + sorted(set([str(word) for seq in train_word for word in seq]))\n",
    "w_to_ix = {ch:i for i,ch in enumerate(w_vocab)}\n",
    "ix_to_w = {i:ch for ch,i in w_to_ix.items()}\n",
    "\n",
    "## create the tag vab\n",
    "t_vocab = ['<pad>'] + sorted(set([str(word) for seq in train_tag for word in seq]))\n",
    "t_to_ix = {ch:i for i,ch in enumerate(t_vocab)}\n",
    "ix_to_t = {i:ch for ch,i in t_to_ix.items()}\n",
    "\n",
    "\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_dim,embedding_dim, hidden_dim, output_dim,n_layers, bidirectional,dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()   \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx = pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                            hidden_dim, \n",
    "                            num_layers = n_layers, \n",
    "                            bidirectional = bidirectional,\n",
    "                            dropout = dropout if n_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.elu = nn.ELU()\n",
    "        \n",
    "    def forward(self,x,batched=True):\n",
    "        if batched:\n",
    "            word_padded = x[0]\n",
    "            word_padded = word_padded.cuda()\n",
    "            tag_padded =x[1]\n",
    "            tag_padded = tag_padded.cuda()\n",
    "            x_lens = x[2]\n",
    "            y_lens = x[3]\n",
    "            #pass text through embedding layer\n",
    "            embedded = self.dropout(self.embedding(word_padded))\n",
    "            x_packed = pack_padded_sequence(embedded, x_lens, batch_first=True, enforce_sorted=False)\n",
    "            #pass packed embeddings into LSTM\n",
    "            outputs, (hidden, cell) = self.lstm(x_packed)\n",
    "            # unpack the padded embeding \n",
    "            outputs, output_lengths = pad_packed_sequence(outputs, batch_first=True)\n",
    "            #we use our outputs to make a prediction of what the tag should be\n",
    "            outputs = self.elu(outputs)\n",
    "            outputs = self.fc(self.dropout(outputs))\n",
    "                 \n",
    "        else:\n",
    "            #pass text through embedding layer\n",
    "            embedded = self.dropout(self.embedding(x))\n",
    "            #pass embeddings into LSTM\n",
    "            outputs, (hidden, cell) = self.lstm(embedded) \n",
    "            #we use our outputs to make a prediction of what the tag should be\n",
    "            outputs = self.elu(outputs)\n",
    "            outputs = self.fc(self.dropout(outputs))\n",
    "                   \n",
    "    \n",
    "        return outputs\n",
    "\n",
    "\n",
    "#initial model with parameters \n",
    "INPUT_DIM = len(w_vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 128\n",
    "N_LAYERS = 1\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.33\n",
    "PAD_IDX = w_vocab.index('<pad>')\n",
    "\n",
    "\n",
    "model = BiLSTM(INPUT_DIM, \n",
    "               EMBEDDING_DIM, \n",
    "               HIDDEN_DIM, \n",
    "               OUTPUT_DIM, \n",
    "               N_LAYERS, \n",
    "               BIDIRECTIONAL, \n",
    "               DROPOUT, \n",
    "               PAD_IDX)\n",
    "\n",
    "\n",
    "\n",
    "### using the model to predict dev\n",
    "## function that find index of each word \n",
    "def finder(x):\n",
    "    token = []\n",
    "    for w in x:\n",
    "        if w in w_to_ix:\n",
    "            vec =w_to_ix[w]\n",
    "        else:\n",
    "            vec = w_to_ix[\"<unk>\"]\n",
    "            \n",
    "        token.append(vec)\n",
    "            \n",
    "    return token \n",
    "\n",
    "### load the best performed model \n",
    "#model.load_state_dict(torch.load('blstm2.pt'))\n",
    "def tag_sentence(model, sentence, w_to_ix, ix_to_t):\n",
    "    \n",
    "    model.eval()\n",
    "   ## convert the word into index \n",
    "    numericalized_tokens = finder(sentence)\n",
    "    ## convert it into tensor \n",
    "    token_tensor = torch.LongTensor(numericalized_tokens)\n",
    "    \n",
    "    token_tensor = token_tensor.unsqueeze(-1)\n",
    "     # use the model to predict     \n",
    "    predictions = model(token_tensor, batched=False)\n",
    "    \n",
    "    top_predictions = predictions.argmax(-1)\n",
    "    \n",
    "    predicted_tags = [ix_to_t[t.item()] for t in top_predictions]\n",
    "    \n",
    "    return predicted_tags\n",
    "\n",
    "\n",
    "\n",
    "### load the model and test on CPU \n",
    "model.load_state_dict(torch.load('blstm2_revised.pt', map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "## READ the dev file \n",
    "dev_file = pd.read_csv('dev', quoting=csv.QUOTE_NONE, sep=\" \", names= ['index', 'word', 'tag'])\n",
    "\n",
    "dev_array = dev_file.to_numpy()\n",
    "\n",
    "dev_word, dev_tag = converter(dev_array)\n",
    "\n",
    "\n",
    "\n",
    "##create the nested list which contain the predicted tagas using our model \n",
    "## predict the dev tag using model for each sentence in dev \n",
    "whole_pred_tags = []\n",
    "\n",
    "for s in dev_word:\n",
    "    \n",
    "    pred_tags= tag_sentence(model, s, w_to_ix, ix_to_t)\n",
    "    \n",
    "    whole_pred_tags.append(pred_tags)\n",
    "\n",
    "\n",
    "def data_to_stream(data):\n",
    "    data_stream = []\n",
    "    i = 0 \n",
    "    for index, word, tag in data:\n",
    "        if index == 1:\n",
    "            temp = []\n",
    "            temp.append([index,word, tag])\n",
    "\n",
    "        else:\n",
    "            temp.append([index ,word,tag ])\n",
    "\n",
    "        if ( (i+1 < len(data)) and data[i+1][0] == 1 ) or (i == len(data)-1):\n",
    "            data_stream.append(temp)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return data_stream \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### to write the output file \n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np \n",
    "import csv\n",
    "dev_file = pd.read_csv('dev', quoting=csv.QUOTE_NONE, sep=\" \", names= ['index', 'word', 'tag'])\n",
    "\n",
    "# convert the file to sentences \n",
    "dev_array = dev_file.to_numpy()\n",
    "dev_stream = data_to_stream(dev_array)\n",
    "\n",
    "\n",
    "def write_out_evl(file_name, data): \n",
    "    with open(file_name,'w') as f:\n",
    "        for i in range(len(data)):\n",
    "            if i != 0:\n",
    "                f.write('\\n')\n",
    "            for (index, w, t) in data[i]:\n",
    "                f.write(str(index))\n",
    "                f.write(' ')\n",
    "                f.write(str(w))\n",
    "                f.write(' ')\n",
    "                f.write(str(t))\n",
    "                f.write(\"\\n\")\n",
    "                \n",
    "#### write the output file with predited tags \n",
    "import copy\n",
    "dev_data_list = copy.deepcopy(dev_stream)\n",
    "for i in range(len(whole_pred_tags)):\n",
    "    for i1 in range(len(whole_pred_tags[i])):\n",
    "        dev_data_list[i][i1].pop() # pop the actual tag \n",
    "        dev_data_list[i][i1].append(whole_pred_tags[i][i1]) # append predicted tag \n",
    "        \n",
    "write_out_evl('dev2.out',dev_data_list)\n",
    "\n",
    "\n",
    "### predict the test tag \n",
    "### first convert the test word into lower case \n",
    "### store the lists of test sentences into a big list \n",
    "def testdata_to_stream(data):\n",
    "    data_stream = []\n",
    "    i = 0\n",
    "    for index, word in data:\n",
    "        if index == 1:\n",
    "            temp = []\n",
    "            temp.append(str(word).lower()) ### convert the word into lower case \n",
    "        else:\n",
    "            temp.append(str(word).lower())\n",
    "        if ( (i+1 < len(data)) and data[i+1][0] == 1 ) or (i == len(data)-1):\n",
    "            data_stream.append(temp)\n",
    "        i += 1\n",
    "    return data_stream\n",
    "\n",
    "\n",
    "# test file \n",
    "test_file = pd.read_csv('test', quoting=csv.QUOTE_NONE, sep=\" \", names= ['index', 'word'])\n",
    "test_data = test_file.to_numpy()\n",
    "test_data = testdata_to_stream(test_data)\n",
    "\n",
    "predicted_test = []\n",
    "for s in test_data:\n",
    "    test_predicted_tags= tag_sentence(model, s, w_to_ix,ix_to_t)\n",
    "    \n",
    "    predicted_test.append(test_predicted_tags)\n",
    "\n",
    "\n",
    "\n",
    "#test_data to test_file\n",
    "def data_t_output(data):\n",
    "    data_stream = []\n",
    "    i = 0\n",
    "    for index, word in data:\n",
    "        if index == 1:\n",
    "            temp = []\n",
    "            temp.append([index, word]) \n",
    "        else:\n",
    "            temp.append([index, word])\n",
    "        if ( (i+1 < len(data)) and data[i+1][0] == 1 ) or (i == len(data)-1):\n",
    "            data_stream.append(temp)\n",
    "        i += 1\n",
    "    return data_stream\n",
    "\n",
    "\n",
    "test_output = data_t_output(test_file.to_numpy())\n",
    "\n",
    "#### write the output file\n",
    "import copy\n",
    "test_data_list = copy.deepcopy(test_output)\n",
    "for i in range(len(predicted_test)):\n",
    "    for i1 in range(len(predicted_test[i])):\n",
    "        test_data_list[i][i1].append(predicted_test[i][i1])\n",
    "\n",
    "\n",
    "#write the test2.out \n",
    "with open('test2.out','w') as f:\n",
    "    for i in range(len(test_data_list)):\n",
    "        if i != 0:\n",
    "            f.write('\\n')\n",
    "        for (i, w, t) in test_data_list[i]:\n",
    "            f.write(str(i))\n",
    "            f.write(\" \")\n",
    "            f.write(str(w))\n",
    "            f.write(\" \")\n",
    "            f.write(str(t))\n",
    "            f.write('\\n')\n",
    "\n",
    "\n",
    "## write the prediction file for eval using perl\n",
    "import copy\n",
    "dev_data_list = copy.deepcopy(dev_stream)\n",
    "for i in range(len(whole_pred_tags)):\n",
    "    for i1 in range(len(whole_pred_tags[i])):\n",
    "        #dev_data_list[i][i1].pop() # pop the actual tag \n",
    "        dev_data_list[i][i1].append(whole_pred_tags[i][i1]) # append predicted tag \n",
    "        \n",
    "def write_prediction(file_name, data):\n",
    "    \n",
    "    with open(file_name,'w') as f:\n",
    "        for i in range(len(data)):\n",
    "            if i !=0:\n",
    "                f.write('\\n')\n",
    "            for (index, w, t,t2) in data[i]:\n",
    "                f.write(str(index))\n",
    "                f.write(' ')\n",
    "                f.write(str(w))\n",
    "                f.write(' ')\n",
    "                f.write(str(t))\n",
    "                f.write(' ')\n",
    "                f.write(str(t2))\n",
    "                f.write(\"\\n\")\n",
    "                \n",
    "write_prediction(\"prediction_task2\", dev_data_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4963730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
