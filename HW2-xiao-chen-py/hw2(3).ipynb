{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20af343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "172eb0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 16148: expected 15 fields, saw 22\\nSkipping line 20100: expected 15 fields, saw 22\\nSkipping line 45178: expected 15 fields, saw 22\\nSkipping line 48700: expected 15 fields, saw 22\\nSkipping line 63331: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 86053: expected 15 fields, saw 22\\nSkipping line 88858: expected 15 fields, saw 22\\nSkipping line 115017: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 137366: expected 15 fields, saw 22\\nSkipping line 139110: expected 15 fields, saw 22\\nSkipping line 165540: expected 15 fields, saw 22\\nSkipping line 171813: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 203723: expected 15 fields, saw 22\\nSkipping line 209366: expected 15 fields, saw 22\\nSkipping line 211310: expected 15 fields, saw 22\\nSkipping line 246351: expected 15 fields, saw 22\\nSkipping line 252364: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 267003: expected 15 fields, saw 22\\nSkipping line 268957: expected 15 fields, saw 22\\nSkipping line 303336: expected 15 fields, saw 22\\nSkipping line 306021: expected 15 fields, saw 22\\nSkipping line 311569: expected 15 fields, saw 22\\nSkipping line 316767: expected 15 fields, saw 22\\nSkipping line 324009: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 359107: expected 15 fields, saw 22\\nSkipping line 368367: expected 15 fields, saw 22\\nSkipping line 381180: expected 15 fields, saw 22\\nSkipping line 390453: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 412243: expected 15 fields, saw 22\\nSkipping line 419342: expected 15 fields, saw 22\\nSkipping line 457388: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 459935: expected 15 fields, saw 22\\nSkipping line 460167: expected 15 fields, saw 22\\nSkipping line 466460: expected 15 fields, saw 22\\nSkipping line 500314: expected 15 fields, saw 22\\nSkipping line 500339: expected 15 fields, saw 22\\nSkipping line 505396: expected 15 fields, saw 22\\nSkipping line 507760: expected 15 fields, saw 22\\nSkipping line 513626: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 527638: expected 15 fields, saw 22\\nSkipping line 534209: expected 15 fields, saw 22\\nSkipping line 535687: expected 15 fields, saw 22\\nSkipping line 547671: expected 15 fields, saw 22\\nSkipping line 549054: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 599929: expected 15 fields, saw 22\\nSkipping line 604776: expected 15 fields, saw 22\\nSkipping line 609937: expected 15 fields, saw 22\\nSkipping line 632059: expected 15 fields, saw 22\\nSkipping line 638546: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 665017: expected 15 fields, saw 22\\nSkipping line 677680: expected 15 fields, saw 22\\nSkipping line 684370: expected 15 fields, saw 22\\nSkipping line 720217: expected 15 fields, saw 29\\n'\n",
      "b'Skipping line 723240: expected 15 fields, saw 22\\nSkipping line 723433: expected 15 fields, saw 22\\nSkipping line 763891: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 800288: expected 15 fields, saw 22\\nSkipping line 802942: expected 15 fields, saw 22\\nSkipping line 803379: expected 15 fields, saw 22\\nSkipping line 805122: expected 15 fields, saw 22\\nSkipping line 821899: expected 15 fields, saw 22\\nSkipping line 831707: expected 15 fields, saw 22\\nSkipping line 842829: expected 15 fields, saw 22\\nSkipping line 843604: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 863904: expected 15 fields, saw 22\\nSkipping line 875655: expected 15 fields, saw 22\\nSkipping line 886796: expected 15 fields, saw 22\\nSkipping line 892299: expected 15 fields, saw 22\\nSkipping line 902518: expected 15 fields, saw 22\\nSkipping line 903079: expected 15 fields, saw 22\\nSkipping line 912678: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 932953: expected 15 fields, saw 22\\nSkipping line 936838: expected 15 fields, saw 22\\nSkipping line 937177: expected 15 fields, saw 22\\nSkipping line 947695: expected 15 fields, saw 22\\nSkipping line 960713: expected 15 fields, saw 22\\nSkipping line 965225: expected 15 fields, saw 22\\nSkipping line 980776: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 999318: expected 15 fields, saw 22\\nSkipping line 1007247: expected 15 fields, saw 22\\nSkipping line 1015987: expected 15 fields, saw 22\\nSkipping line 1018984: expected 15 fields, saw 22\\nSkipping line 1028671: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1063360: expected 15 fields, saw 22\\nSkipping line 1066195: expected 15 fields, saw 22\\nSkipping line 1066578: expected 15 fields, saw 22\\nSkipping line 1066869: expected 15 fields, saw 22\\nSkipping line 1068809: expected 15 fields, saw 22\\nSkipping line 1069505: expected 15 fields, saw 22\\nSkipping line 1087983: expected 15 fields, saw 22\\nSkipping line 1108184: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1118137: expected 15 fields, saw 22\\nSkipping line 1142723: expected 15 fields, saw 22\\nSkipping line 1152492: expected 15 fields, saw 22\\nSkipping line 1156947: expected 15 fields, saw 22\\nSkipping line 1172563: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1209254: expected 15 fields, saw 22\\nSkipping line 1212966: expected 15 fields, saw 22\\nSkipping line 1236533: expected 15 fields, saw 22\\nSkipping line 1237598: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1273825: expected 15 fields, saw 22\\nSkipping line 1277898: expected 15 fields, saw 22\\nSkipping line 1283654: expected 15 fields, saw 22\\nSkipping line 1286023: expected 15 fields, saw 22\\nSkipping line 1302038: expected 15 fields, saw 22\\nSkipping line 1305179: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1326022: expected 15 fields, saw 22\\nSkipping line 1338120: expected 15 fields, saw 22\\nSkipping line 1338503: expected 15 fields, saw 22\\nSkipping line 1338849: expected 15 fields, saw 22\\nSkipping line 1341513: expected 15 fields, saw 22\\nSkipping line 1346493: expected 15 fields, saw 22\\nSkipping line 1373127: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1389508: expected 15 fields, saw 22\\nSkipping line 1413951: expected 15 fields, saw 22\\nSkipping line 1433626: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1442698: expected 15 fields, saw 22\\nSkipping line 1472982: expected 15 fields, saw 22\\nSkipping line 1482282: expected 15 fields, saw 22\\nSkipping line 1487808: expected 15 fields, saw 22\\nSkipping line 1500636: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1511479: expected 15 fields, saw 22\\nSkipping line 1532302: expected 15 fields, saw 22\\nSkipping line 1537952: expected 15 fields, saw 22\\nSkipping line 1539951: expected 15 fields, saw 22\\nSkipping line 1541020: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1594217: expected 15 fields, saw 22\\nSkipping line 1612264: expected 15 fields, saw 22\\nSkipping line 1615907: expected 15 fields, saw 22\\nSkipping line 1621859: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1653542: expected 15 fields, saw 22\\nSkipping line 1671537: expected 15 fields, saw 22\\nSkipping line 1672879: expected 15 fields, saw 22\\nSkipping line 1674523: expected 15 fields, saw 22\\nSkipping line 1677355: expected 15 fields, saw 22\\nSkipping line 1703907: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1713046: expected 15 fields, saw 22\\nSkipping line 1722982: expected 15 fields, saw 22\\nSkipping line 1727290: expected 15 fields, saw 22\\nSkipping line 1744482: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1803858: expected 15 fields, saw 22\\nSkipping line 1810069: expected 15 fields, saw 22\\nSkipping line 1829751: expected 15 fields, saw 22\\nSkipping line 1831699: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1863131: expected 15 fields, saw 22\\nSkipping line 1867917: expected 15 fields, saw 22\\nSkipping line 1874790: expected 15 fields, saw 22\\nSkipping line 1879952: expected 15 fields, saw 22\\nSkipping line 1880501: expected 15 fields, saw 22\\nSkipping line 1886655: expected 15 fields, saw 22\\nSkipping line 1887888: expected 15 fields, saw 22\\nSkipping line 1894286: expected 15 fields, saw 22\\nSkipping line 1895400: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1904040: expected 15 fields, saw 22\\nSkipping line 1907604: expected 15 fields, saw 22\\nSkipping line 1915739: expected 15 fields, saw 22\\nSkipping line 1921514: expected 15 fields, saw 22\\nSkipping line 1939428: expected 15 fields, saw 22\\nSkipping line 1944342: expected 15 fields, saw 22\\nSkipping line 1949699: expected 15 fields, saw 22\\nSkipping line 1961872: expected 15 fields, saw 22\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 1968846: expected 15 fields, saw 22\\nSkipping line 1999941: expected 15 fields, saw 22\\nSkipping line 2001492: expected 15 fields, saw 22\\nSkipping line 2011204: expected 15 fields, saw 22\\nSkipping line 2025295: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2041266: expected 15 fields, saw 22\\nSkipping line 2073314: expected 15 fields, saw 22\\nSkipping line 2080133: expected 15 fields, saw 22\\nSkipping line 2088521: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2103490: expected 15 fields, saw 22\\nSkipping line 2115278: expected 15 fields, saw 22\\nSkipping line 2153174: expected 15 fields, saw 22\\nSkipping line 2161731: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2165250: expected 15 fields, saw 22\\nSkipping line 2175132: expected 15 fields, saw 22\\nSkipping line 2206817: expected 15 fields, saw 22\\nSkipping line 2215848: expected 15 fields, saw 22\\nSkipping line 2223811: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2257265: expected 15 fields, saw 22\\nSkipping line 2259163: expected 15 fields, saw 22\\nSkipping line 2263291: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2301943: expected 15 fields, saw 22\\nSkipping line 2304371: expected 15 fields, saw 22\\nSkipping line 2306015: expected 15 fields, saw 22\\nSkipping line 2312186: expected 15 fields, saw 22\\nSkipping line 2314740: expected 15 fields, saw 22\\nSkipping line 2317754: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2383514: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2449763: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2589323: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2775036: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2935174: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3078830: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3123091: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3185533: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 4150395: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 4748401: expected 15 fields, saw 22\\n'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv(\"amazon_reviews_us_Kitchen_v1_00.tsv\", sep = '\\t',error_bad_lines = False)\n",
    "test['label'] = -1\n",
    "a1 = test.loc[test['star_rating']==1].sample(50000)\n",
    "a2 = test.loc[test['star_rating']==2].sample(50000)\n",
    "a3 = test.loc[test['star_rating']==3].sample(50000)\n",
    "a4 = test.loc[test['star_rating']==4].sample(50000)\n",
    "a5 = test.loc[test['star_rating']==5].sample(50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "952fea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test = pd.concat([a1,a2,a3,a4,a5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f64b5b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test['lable'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adbf99b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "new_test.label[test.star_rating>3] = 1\n",
    "new_test.label[test.star_rating<3] = 2\n",
    "new_test.label[test.star_rating==3] = 3\n",
    "new_test = new_test[['label','review_body']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ab0c396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model_1 = gensim.models.Word2Vec.load('save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cfe6721",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data clearing\n",
    "new_test['review_body'] = new_test['review_body'].str.lower()\n",
    "\n",
    "def tag(x):\n",
    "    return re.sub('<.*?>','',str(x))\n",
    "new_test['review_body'] = new_test['review_body'].apply(lambda x:tag(x))\n",
    "\n",
    "def url(x):\n",
    "    return re.sub('(https?|ftp|file)://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]','',str(x))\n",
    "\n",
    "new_test['review_body'] = new_test['review_body'].apply(lambda x:url(x))\n",
    "\n",
    "import contractions\n",
    "new_test['review_body'] = new_test['review_body'].apply(lambda x:contractions.fix(x))\n",
    "\n",
    "def non_alphabetical(x):\n",
    "    return re.sub('[^a-zA-Z\\s]','',str(x))\n",
    "new_test['review_body'] = new_test['review_body'].apply(lambda x:non_alphabetical(x))\n",
    "def extra_space(x):\n",
    "    return re.sub( ' +',' ',str(x))\n",
    "new_test['review_body'] = new_test['review_body'].apply(lambda x:extra_space(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad7850e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chenlin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words_set = set(stopwords.words('english'))\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def stop_words(x):\n",
    "    word_tokens = word_tokenize(x)\n",
    "    temp = []\n",
    "    for i in word_tokens:\n",
    "        if i not in stop_words_set:\n",
    "            temp.append(i)\n",
    "    return temp\n",
    "new_test['review_body'] = new_test['review_body'].apply(lambda x:stop_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6854388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1261010</th>\n",
       "      <td>2</td>\n",
       "      <td>[victorinox, product, first, word, read]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149938</th>\n",
       "      <td>2</td>\n",
       "      <td>[match, size, color, tho, color, name, also, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4379464</th>\n",
       "      <td>2</td>\n",
       "      <td>[advertised, add, char, broil, cooker, purchas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28297</th>\n",
       "      <td>2</td>\n",
       "      <td>[yuck]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844336</th>\n",
       "      <td>2</td>\n",
       "      <td>[bought, toaster, black, decker, name, sorry, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3962262</th>\n",
       "      <td>1</td>\n",
       "      <td>[well, made, lid, stays, tight, else, ask, mug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501108</th>\n",
       "      <td>1</td>\n",
       "      <td>[love, dishes, added, coffee, cups, set, years]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2533959</th>\n",
       "      <td>1</td>\n",
       "      <td>[pug, lover, like, love, cool, fridge, magnet,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48365</th>\n",
       "      <td>1</td>\n",
       "      <td>[bought, pan, make, cake, yo, celebrate, offic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4145163</th>\n",
       "      <td>1</td>\n",
       "      <td>[person, gave, toaster, oven, bad, review, sai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                        review_body\n",
       "1261010      2           [victorinox, product, first, word, read]\n",
       "149938       2  [match, size, color, tho, color, name, also, e...\n",
       "4379464      2  [advertised, add, char, broil, cooker, purchas...\n",
       "28297        2                                             [yuck]\n",
       "4844336      2  [bought, toaster, black, decker, name, sorry, ...\n",
       "...        ...                                                ...\n",
       "3962262      1  [well, made, lid, stays, tight, else, ask, mug...\n",
       "1501108      1    [love, dishes, added, coffee, cups, set, years]\n",
       "2533959      1  [pug, lover, like, love, cool, fridge, magnet,...\n",
       "48365        1  [bought, pan, make, cake, yo, celebrate, offic...\n",
       "4145163      1  [person, gave, toaster, oven, bad, review, sai...\n",
       "\n",
       "[250000 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4a75ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_avg(x):\n",
    "    i = 0\n",
    "    n = 0\n",
    "    a = np.zeros(300)\n",
    "    while i<len(x):\n",
    "        if x[i] in model_1.wv:\n",
    "            a += model_1.wv[x[i]]\n",
    "        else:\n",
    "            n += 1\n",
    "        i = i+1\n",
    "    if n == 0: return a\n",
    "    else:return a/(len(x)-n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "613092cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test['avg_vector'] = new_test['review_body'].apply(lambda x:vector_avg(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d0a7669",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test['y'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9a54b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "new_test['y'][new_test['label'] == 1] =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a31c50e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "new_test['y'][new_test['label'] == 2] =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6912bb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "new_test['y'][new_test['label'] == 3] =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d11680d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review_body</th>\n",
       "      <th>avg_vector</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1261010</th>\n",
       "      <td>2</td>\n",
       "      <td>[victorinox, product, first, word, read]</td>\n",
       "      <td>[0.3361993785947561, 0.3755444842390716, 0.205...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149938</th>\n",
       "      <td>2</td>\n",
       "      <td>[match, size, color, tho, color, name, also, e...</td>\n",
       "      <td>[0.023530922830104828, 0.7736800406128168, 0.0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4379464</th>\n",
       "      <td>2</td>\n",
       "      <td>[advertised, add, char, broil, cooker, purchas...</td>\n",
       "      <td>[0.9732713364064693, 1.8002999844029546, 0.767...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28297</th>\n",
       "      <td>2</td>\n",
       "      <td>[yuck]</td>\n",
       "      <td>[-0.30345696210861206, 0.005608719307929277, -...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844336</th>\n",
       "      <td>2</td>\n",
       "      <td>[bought, toaster, black, decker, name, sorry, ...</td>\n",
       "      <td>[0.05253013025219666, 0.0007164585744825805, 0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3962262</th>\n",
       "      <td>1</td>\n",
       "      <td>[well, made, lid, stays, tight, else, ask, mug...</td>\n",
       "      <td>[0.6963680814951658, 1.8281475193798542, -0.74...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501108</th>\n",
       "      <td>1</td>\n",
       "      <td>[love, dishes, added, coffee, cups, set, years]</td>\n",
       "      <td>[1.1396409068256617, 0.5796800553798676, 0.723...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2533959</th>\n",
       "      <td>1</td>\n",
       "      <td>[pug, lover, like, love, cool, fridge, magnet,...</td>\n",
       "      <td>[0.031026512011885644, 0.06857175541420778, -0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48365</th>\n",
       "      <td>1</td>\n",
       "      <td>[bought, pan, make, cake, yo, celebrate, offic...</td>\n",
       "      <td>[0.04685636181134863, 0.04049028242713705, -0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4145163</th>\n",
       "      <td>1</td>\n",
       "      <td>[person, gave, toaster, oven, bad, review, sai...</td>\n",
       "      <td>[0.06293770963466673, 0.027035569575536152, -0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                        review_body  \\\n",
       "1261010      2           [victorinox, product, first, word, read]   \n",
       "149938       2  [match, size, color, tho, color, name, also, e...   \n",
       "4379464      2  [advertised, add, char, broil, cooker, purchas...   \n",
       "28297        2                                             [yuck]   \n",
       "4844336      2  [bought, toaster, black, decker, name, sorry, ...   \n",
       "...        ...                                                ...   \n",
       "3962262      1  [well, made, lid, stays, tight, else, ask, mug...   \n",
       "1501108      1    [love, dishes, added, coffee, cups, set, years]   \n",
       "2533959      1  [pug, lover, like, love, cool, fridge, magnet,...   \n",
       "48365        1  [bought, pan, make, cake, yo, celebrate, offic...   \n",
       "4145163      1  [person, gave, toaster, oven, bad, review, sai...   \n",
       "\n",
       "                                                avg_vector  y  \n",
       "1261010  [0.3361993785947561, 0.3755444842390716, 0.205...  1  \n",
       "149938   [0.023530922830104828, 0.7736800406128168, 0.0...  1  \n",
       "4379464  [0.9732713364064693, 1.8002999844029546, 0.767...  1  \n",
       "28297    [-0.30345696210861206, 0.005608719307929277, -...  1  \n",
       "4844336  [0.05253013025219666, 0.0007164585744825805, 0...  1  \n",
       "...                                                    ... ..  \n",
       "3962262  [0.6963680814951658, 1.8281475193798542, -0.74...  0  \n",
       "1501108  [1.1396409068256617, 0.5796800553798676, 0.723...  0  \n",
       "2533959  [0.031026512011885644, 0.06857175541420778, -0...  0  \n",
       "48365    [0.04685636181134863, 0.04049028242713705, -0....  0  \n",
       "4145163  [0.06293770963466673, 0.027035569575536152, -0...  0  \n",
       "\n",
       "[250000 rows x 4 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7de14b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_test['avg_vector']\n",
    "Y = new_test['y']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state = 19, test_size = 0.2)\n",
    "\n",
    "x_train = np.array(x_train.tolist())\n",
    "\n",
    "y_train = np.array(y_train.tolist())\n",
    "\n",
    "x_train = np.nan_to_num(x_train)\n",
    "\n",
    "x_test = np.array(x_test.tolist())\n",
    "\n",
    "y_test = np.array(y_test.tolist())\n",
    "\n",
    "x_test = np.nan_to_num(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443dc8a6",
   "metadata": {},
   "source": [
    "# ternary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2bb45205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05ad9b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c90142ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of training set to use as validation\n",
    "#valid_size = 0.2\n",
    "\n",
    "DatasetTrain = TensorDataset(torch.from_numpy(x_train),torch.from_numpy(y_train))\n",
    "        \n",
    "DatasetTest=TensorDataset(torch.from_numpy(x_test),torch.from_numpy(y_test))\n",
    "\n",
    "train_loader=torch.utils.data.DataLoader(DatasetTrain,batch_size=batch_size,shuffle=True,drop_last=True, num_workers=0)\n",
    "\n",
    "valid_loader=torch.utils.data.DataLoader(DatasetTest, batch_size=batch_size, drop_last=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7c578ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.4691e+00,  2.8362e+00,  6.5291e-01,  ...,  1.4724e+00,\n",
      "          4.6802e+00, -3.3048e+00],\n",
      "        [ 8.3865e-01,  1.8521e-01,  1.7748e-01,  ...,  1.5910e+00,\n",
      "          1.3063e+00, -8.6692e-01],\n",
      "        [-2.9935e-01,  5.9200e-01,  2.7225e-01,  ...,  7.0387e-01,\n",
      "          5.6117e-01,  5.3711e-01],\n",
      "        ...,\n",
      "        [ 7.1137e-01,  1.1082e+00,  9.6256e-01,  ...,  1.1243e+00,\n",
      "          1.8219e+00, -1.9473e-02],\n",
      "        [-9.6564e-01,  2.0634e+00,  4.8056e-01,  ...,  1.2328e+00,\n",
      "          1.0309e+00, -1.7420e+00],\n",
      "        [ 3.0430e-02,  6.6379e-02,  4.6200e-03,  ...,  1.9586e-02,\n",
      "          7.6102e-02, -6.7758e-02]], dtype=torch.float64) tensor([1, 0, 1, 0, 0, 1, 2, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for xi ,yi in train_loader:\n",
    "    print(xi,yi)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52663a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # number of hidden nodes in each layer \n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        # linear layer (300 -> hidden_1)\n",
    "        self.fc1 = nn.Linear(300, hidden_1)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        # linear layer (n_hidden -> 10)\n",
    "        self.fc3 = nn.Linear(hidden_2, 3)\n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 300)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add output layer\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6330e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "## Specify loss and optimization functions\n",
    "\n",
    "# specify loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c364bb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.830280\n",
      "Epoch: 2 \tTraining Loss: 0.778383\n",
      "Epoch: 3 \tTraining Loss: 0.760423\n",
      "Epoch: 4 \tTraining Loss: 0.752586\n",
      "Epoch: 5 \tTraining Loss: 0.747508\n",
      "Epoch: 6 \tTraining Loss: 0.743031\n",
      "Epoch: 7 \tTraining Loss: 0.739228\n",
      "Epoch: 8 \tTraining Loss: 0.735913\n",
      "Epoch: 9 \tTraining Loss: 0.733647\n",
      "Epoch: 10 \tTraining Loss: 0.731600\n",
      "Epoch: 11 \tTraining Loss: 0.729493\n",
      "Epoch: 12 \tTraining Loss: 0.727222\n",
      "Epoch: 13 \tTraining Loss: 0.725659\n",
      "Epoch: 14 \tTraining Loss: 0.726512\n",
      "Epoch: 15 \tTraining Loss: 0.723097\n",
      "Epoch: 16 \tTraining Loss: 0.722527\n",
      "Epoch: 17 \tTraining Loss: 0.721406\n",
      "Epoch: 18 \tTraining Loss: 0.719671\n",
      "Epoch: 19 \tTraining Loss: 0.717970\n",
      "Epoch: 20 \tTraining Loss: 0.717478\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 20  # suggest training between 20-50 epochs\n",
    "\n",
    "model.train() # prep model for training\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data.float())\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "83344ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DatasetTest=TensorDataset(torch.from_numpy(x_test),torch.from_numpy(y_test))\n",
    "\n",
    "total_num = 0\n",
    "correct = 0\n",
    "for x_temp,y_real in DatasetTest:\n",
    "    res = model(x_temp.float())\n",
    "    _, pred = torch.max(res,1)\n",
    "    total_num += 1\n",
    "    if int(pred) == int(y_real):\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e2a7c486",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_mymodel_avg =  correct/total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "34908903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for ternary label from my model using average vector is  0.6855\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy for ternary label from my model using average vector is ', accuracy_mymodel_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3711651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_concat(x):\n",
    "    i = 0\n",
    "    n_x = 0\n",
    "    length = len(x)\n",
    "    if length == 0:\n",
    "        return np.zeros(3000)\n",
    "    a = np.zeros(300)\n",
    "    \n",
    "    while i<10:\n",
    "        if n_x < length:\n",
    "            if x[n_x] in model_1.wv :\n",
    "                if i == 0:\n",
    "                    ans = copy.deepcopy(model_1.wv[x[n_x]])\n",
    "                    i += 1\n",
    "                else:\n",
    "                    ans = np.concatenate((ans,model_1.wv[x[n_x]]), axis=None)\n",
    "                    i += 1\n",
    "            n_x += 1\n",
    "        else:\n",
    "            if i == 0:\n",
    "                ans = a\n",
    "                i += 1\n",
    "            else:\n",
    "                ans = np.concatenate((ans,a), axis=None)\n",
    "                i += 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "552261c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test['concat_vec'] = new_test['review_body'].apply(lambda x:vector_concat(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "82e4a40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review_body</th>\n",
       "      <th>avg_vector</th>\n",
       "      <th>y</th>\n",
       "      <th>concat_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1261010</th>\n",
       "      <td>2</td>\n",
       "      <td>[victorinox, product, first, word, read]</td>\n",
       "      <td>[0.3361993785947561, 0.3755444842390716, 0.205...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.11119204014539719, -0.04113255813717842, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149938</th>\n",
       "      <td>2</td>\n",
       "      <td>[match, size, color, tho, color, name, also, e...</td>\n",
       "      <td>[0.023530922830104828, 0.7736800406128168, 0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.15528874, 0.082478374, 0.16331409, 0.11810...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4379464</th>\n",
       "      <td>2</td>\n",
       "      <td>[advertised, add, char, broil, cooker, purchas...</td>\n",
       "      <td>[0.9732713364064693, 1.8002999844029546, 0.767...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0612764, 0.18021023, -0.078093216, -0.21873...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28297</th>\n",
       "      <td>2</td>\n",
       "      <td>[yuck]</td>\n",
       "      <td>[-0.30345696210861206, 0.005608719307929277, -...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.30345696210861206, 0.005608719307929277, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844336</th>\n",
       "      <td>2</td>\n",
       "      <td>[bought, toaster, black, decker, name, sorry, ...</td>\n",
       "      <td>[0.05253013025219666, 0.0007164585744825805, 0...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.06608908, -0.18412489, 0.31485054, -0.13780...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3962262</th>\n",
       "      <td>1</td>\n",
       "      <td>[well, made, lid, stays, tight, else, ask, mug...</td>\n",
       "      <td>[0.6963680814951658, 1.8281475193798542, -0.74...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.023777505, 0.19637382, 0.094618395, 0.0448...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501108</th>\n",
       "      <td>1</td>\n",
       "      <td>[love, dishes, added, coffee, cups, set, years]</td>\n",
       "      <td>[1.1396409068256617, 0.5796800553798676, 0.723...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.22806349396705627, 0.019244728609919548, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2533959</th>\n",
       "      <td>1</td>\n",
       "      <td>[pug, lover, like, love, cool, fridge, magnet,...</td>\n",
       "      <td>[0.031026512011885644, 0.06857175541420778, -0...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.41755006, 0.32110247, 0.17386933, 0.1215716...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48365</th>\n",
       "      <td>1</td>\n",
       "      <td>[bought, pan, make, cake, yo, celebrate, offic...</td>\n",
       "      <td>[0.04685636181134863, 0.04049028242713705, -0....</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.06608908, -0.18412489, 0.31485054, -0.13780...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4145163</th>\n",
       "      <td>1</td>\n",
       "      <td>[person, gave, toaster, oven, bad, review, sai...</td>\n",
       "      <td>[0.06293770963466673, 0.027035569575536152, -0...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.16218725, 0.10329889, -0.3650812, -0.031553...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                        review_body  \\\n",
       "1261010      2           [victorinox, product, first, word, read]   \n",
       "149938       2  [match, size, color, tho, color, name, also, e...   \n",
       "4379464      2  [advertised, add, char, broil, cooker, purchas...   \n",
       "28297        2                                             [yuck]   \n",
       "4844336      2  [bought, toaster, black, decker, name, sorry, ...   \n",
       "...        ...                                                ...   \n",
       "3962262      1  [well, made, lid, stays, tight, else, ask, mug...   \n",
       "1501108      1    [love, dishes, added, coffee, cups, set, years]   \n",
       "2533959      1  [pug, lover, like, love, cool, fridge, magnet,...   \n",
       "48365        1  [bought, pan, make, cake, yo, celebrate, offic...   \n",
       "4145163      1  [person, gave, toaster, oven, bad, review, sai...   \n",
       "\n",
       "                                                avg_vector  y  \\\n",
       "1261010  [0.3361993785947561, 0.3755444842390716, 0.205...  1   \n",
       "149938   [0.023530922830104828, 0.7736800406128168, 0.0...  1   \n",
       "4379464  [0.9732713364064693, 1.8002999844029546, 0.767...  1   \n",
       "28297    [-0.30345696210861206, 0.005608719307929277, -...  1   \n",
       "4844336  [0.05253013025219666, 0.0007164585744825805, 0...  1   \n",
       "...                                                    ... ..   \n",
       "3962262  [0.6963680814951658, 1.8281475193798542, -0.74...  0   \n",
       "1501108  [1.1396409068256617, 0.5796800553798676, 0.723...  0   \n",
       "2533959  [0.031026512011885644, 0.06857175541420778, -0...  0   \n",
       "48365    [0.04685636181134863, 0.04049028242713705, -0....  0   \n",
       "4145163  [0.06293770963466673, 0.027035569575536152, -0...  0   \n",
       "\n",
       "                                                concat_vec  \n",
       "1261010  [0.11119204014539719, -0.04113255813717842, 0....  \n",
       "149938   [-0.15528874, 0.082478374, 0.16331409, 0.11810...  \n",
       "4379464  [0.0612764, 0.18021023, -0.078093216, -0.21873...  \n",
       "28297    [-0.30345696210861206, 0.005608719307929277, -...  \n",
       "4844336  [0.06608908, -0.18412489, 0.31485054, -0.13780...  \n",
       "...                                                    ...  \n",
       "3962262  [-0.023777505, 0.19637382, 0.094618395, 0.0448...  \n",
       "1501108  [0.22806349396705627, 0.019244728609919548, 0....  \n",
       "2533959  [0.41755006, 0.32110247, 0.17386933, 0.1215716...  \n",
       "48365    [0.06608908, -0.18412489, 0.31485054, -0.13780...  \n",
       "4145163  [0.16218725, 0.10329889, -0.3650812, -0.031553...  \n",
       "\n",
       "[250000 rows x 5 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "df6c30d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_test['concat_vec']\n",
    "Y = new_test['y']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state = 19, test_size = 0.2)\n",
    "\n",
    "x_train = np.array(x_train.tolist())\n",
    "\n",
    "y_train = np.array(y_train.tolist())\n",
    "\n",
    "x_train = np.nan_to_num(x_train)\n",
    "\n",
    "x_test = np.array(x_test.tolist())\n",
    "\n",
    "y_test = np.array(y_test.tolist())\n",
    "\n",
    "x_test = np.nan_to_num(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2a282ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5b3464fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of training set to use as validation\n",
    "#valid_size = 0.2\n",
    "\n",
    "DatasetTrain = TensorDataset(torch.from_numpy(x_train),torch.from_numpy(y_train))\n",
    "        \n",
    "DatasetTest=TensorDataset(torch.from_numpy(x_test),torch.from_numpy(y_test))\n",
    "\n",
    "train_loader=torch.utils.data.DataLoader(DatasetTrain,batch_size=batch_size,shuffle=True,drop_last=True, num_workers=0)\n",
    "\n",
    "valid_loader=torch.utils.data.DataLoader(DatasetTest, batch_size=batch_size, drop_last=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4c7c1589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # number of hidden nodes in each layer \n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        # linear layer (300 -> hidden_1)\n",
    "        self.fc1 = nn.Linear(3000, hidden_1)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        # linear layer (n_hidden -> 10)\n",
    "        self.fc3 = nn.Linear(hidden_2, 3)\n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 3000)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add output layer\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b633be6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "## Specify loss and optimization functions\n",
    "\n",
    "# specify loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d3817d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.919045\n",
      "Epoch: 2 \tTraining Loss: 0.838901\n",
      "Epoch: 3 \tTraining Loss: 0.816031\n",
      "Epoch: 4 \tTraining Loss: 0.799537\n",
      "Epoch: 5 \tTraining Loss: 0.785014\n",
      "Epoch: 6 \tTraining Loss: 0.770246\n",
      "Epoch: 7 \tTraining Loss: 0.757134\n",
      "Epoch: 8 \tTraining Loss: 0.744034\n",
      "Epoch: 9 \tTraining Loss: 0.730689\n",
      "Epoch: 10 \tTraining Loss: 0.717832\n",
      "Epoch: 11 \tTraining Loss: 0.704430\n",
      "Epoch: 12 \tTraining Loss: 0.692234\n",
      "Epoch: 13 \tTraining Loss: 0.681463\n",
      "Epoch: 14 \tTraining Loss: 0.670074\n",
      "Epoch: 15 \tTraining Loss: 0.660044\n",
      "Epoch: 16 \tTraining Loss: 0.648021\n",
      "Epoch: 17 \tTraining Loss: 0.640992\n",
      "Epoch: 18 \tTraining Loss: 0.630216\n",
      "Epoch: 19 \tTraining Loss: 0.623205\n",
      "Epoch: 20 \tTraining Loss: 0.614358\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 20  # suggest training between 20-50 epochs\n",
    "\n",
    "model.train() # prep model for training\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data.float())\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5f7db3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "DatasetTest=TensorDataset(torch.from_numpy(x_test),torch.from_numpy(y_test))\n",
    "\n",
    "total_num = 0\n",
    "correct = 0\n",
    "for x_temp,y_real in DatasetTest:\n",
    "    res = model(x_temp.float())\n",
    "    _, pred = torch.max(res,1)\n",
    "    total_num += 1\n",
    "    if int(pred) == int(y_real):\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4679f7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_mymodel_concat_ternary =  correct/total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2b7dd41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for ternary label from my model using concat vector is  0.61106\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy for ternary label from my model using concat vector is ', accuracy_mymodel_concat_ternary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f6cfb6",
   "metadata": {},
   "source": [
    "# google news 300 for ternary label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd1eef2",
   "metadata": {},
   "source": [
    "# average vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "52ecf84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d9b087f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "googel_300 = new_test[['label','review_body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0e03196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_concat(x):\n",
    "    i = 0\n",
    "    n_x = 0\n",
    "    length = len(x)\n",
    "    if length == 0:\n",
    "        return np.zeros(3000)\n",
    "    a = np.zeros(300)\n",
    "    \n",
    "    while i<10:\n",
    "        if n_x < length:\n",
    "            if x[n_x] in wv :\n",
    "                if i == 0:\n",
    "                    ans = wv[x[n_x]]\n",
    "                    i += 1\n",
    "                else:\n",
    "                    ans = np.concatenate((ans,wv[x[n_x]]), axis=None)\n",
    "                    i += 1\n",
    "            n_x += 1\n",
    "        else:\n",
    "            if i == 0:\n",
    "                ans = a\n",
    "                i += 1\n",
    "            else:\n",
    "                ans = np.concatenate((ans,a), axis=None)\n",
    "                i += 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "06cbd38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "googel_300['concat_vec'] = googel_300['review_body'].apply(lambda x:vector_concat(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "65401e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_avg(x):\n",
    "    i = 0\n",
    "    n = 0\n",
    "    a = np.zeros(300)\n",
    "    while i<len(x):\n",
    "        if x[i] in wv:\n",
    "            a += wv[x[i]]\n",
    "        else:\n",
    "            n += 1\n",
    "        i = i+1\n",
    "    if n == 0: return a\n",
    "    else:return a/(len(x)-n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ecf3ad19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "googel_300['avg_vector'] = googel_300['review_body'].apply(lambda x:vector_avg(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "942a6068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "googel_300['y'] = -1\n",
    "googel_300['y'] [googel_300['label']== 1] = 0\n",
    "googel_300['y'] [googel_300['label']== 2] = 1\n",
    "googel_300['y'] [googel_300['label']== 3] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "04e91422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review_body</th>\n",
       "      <th>concat_vec</th>\n",
       "      <th>avg_vector</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1261010</th>\n",
       "      <td>2</td>\n",
       "      <td>[victorinox, product, first, word, read]</td>\n",
       "      <td>[-0.0615234375, 0.09521484375, 0.1337890625, 0...</td>\n",
       "      <td>[0.09716796875, -0.015625, 0.033599853515625, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149938</th>\n",
       "      <td>2</td>\n",
       "      <td>[match, size, color, tho, color, name, also, e...</td>\n",
       "      <td>[-0.15527344, 0.025024414, 0.064941406, -0.124...</td>\n",
       "      <td>[-0.073455810546875, 0.9627685546875, -0.23001...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4379464</th>\n",
       "      <td>2</td>\n",
       "      <td>[advertised, add, char, broil, cooker, purchas...</td>\n",
       "      <td>[-0.044433594, -0.017456055, -0.10986328, -0.1...</td>\n",
       "      <td>[0.16021728515625, 0.9654693603515625, -0.6411...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28297</th>\n",
       "      <td>2</td>\n",
       "      <td>[yuck]</td>\n",
       "      <td>[-0.09765625, 0.1728515625, 0.25, 0.3515625, -...</td>\n",
       "      <td>[-0.09765625, 0.1728515625, 0.25, 0.3515625, -...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844336</th>\n",
       "      <td>2</td>\n",
       "      <td>[bought, toaster, black, decker, name, sorry, ...</td>\n",
       "      <td>[0.16699219, -0.05419922, -0.087402344, 0.0196...</td>\n",
       "      <td>[3.5324554443359375, 0.277191162109375, 2.2111...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3962262</th>\n",
       "      <td>1</td>\n",
       "      <td>[well, made, lid, stays, tight, else, ask, mug...</td>\n",
       "      <td>[-0.08251953, 0.022460938, -0.14941406, 0.0991...</td>\n",
       "      <td>[0.4085693359375, 1.667144775390625, -0.440200...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501108</th>\n",
       "      <td>1</td>\n",
       "      <td>[love, dishes, added, coffee, cups, set, years]</td>\n",
       "      <td>[0.10302734375, -0.15234375, 0.02587890625, 0....</td>\n",
       "      <td>[-0.83380126953125, 0.60302734375, -0.04724121...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2533959</th>\n",
       "      <td>1</td>\n",
       "      <td>[pug, lover, like, love, cool, fridge, magnet,...</td>\n",
       "      <td>[-0.07128906, -0.013244629, -0.16503906, 0.197...</td>\n",
       "      <td>[0.7264404296875, 0.86065673828125, -0.7277069...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48365</th>\n",
       "      <td>1</td>\n",
       "      <td>[bought, pan, make, cake, yo, celebrate, offic...</td>\n",
       "      <td>[0.16699219, -0.05419922, -0.087402344, 0.0196...</td>\n",
       "      <td>[-0.018688746861049106, 0.016660417829241072, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4145163</th>\n",
       "      <td>1</td>\n",
       "      <td>[person, gave, toaster, oven, bad, review, sai...</td>\n",
       "      <td>[0.27539062, -0.24707031, 0.017211914, 0.16796...</td>\n",
       "      <td>[-0.03807124385127315, 0.061185483579282406, 0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                        review_body  \\\n",
       "1261010      2           [victorinox, product, first, word, read]   \n",
       "149938       2  [match, size, color, tho, color, name, also, e...   \n",
       "4379464      2  [advertised, add, char, broil, cooker, purchas...   \n",
       "28297        2                                             [yuck]   \n",
       "4844336      2  [bought, toaster, black, decker, name, sorry, ...   \n",
       "...        ...                                                ...   \n",
       "3962262      1  [well, made, lid, stays, tight, else, ask, mug...   \n",
       "1501108      1    [love, dishes, added, coffee, cups, set, years]   \n",
       "2533959      1  [pug, lover, like, love, cool, fridge, magnet,...   \n",
       "48365        1  [bought, pan, make, cake, yo, celebrate, offic...   \n",
       "4145163      1  [person, gave, toaster, oven, bad, review, sai...   \n",
       "\n",
       "                                                concat_vec  \\\n",
       "1261010  [-0.0615234375, 0.09521484375, 0.1337890625, 0...   \n",
       "149938   [-0.15527344, 0.025024414, 0.064941406, -0.124...   \n",
       "4379464  [-0.044433594, -0.017456055, -0.10986328, -0.1...   \n",
       "28297    [-0.09765625, 0.1728515625, 0.25, 0.3515625, -...   \n",
       "4844336  [0.16699219, -0.05419922, -0.087402344, 0.0196...   \n",
       "...                                                    ...   \n",
       "3962262  [-0.08251953, 0.022460938, -0.14941406, 0.0991...   \n",
       "1501108  [0.10302734375, -0.15234375, 0.02587890625, 0....   \n",
       "2533959  [-0.07128906, -0.013244629, -0.16503906, 0.197...   \n",
       "48365    [0.16699219, -0.05419922, -0.087402344, 0.0196...   \n",
       "4145163  [0.27539062, -0.24707031, 0.017211914, 0.16796...   \n",
       "\n",
       "                                                avg_vector  y  \n",
       "1261010  [0.09716796875, -0.015625, 0.033599853515625, ...  1  \n",
       "149938   [-0.073455810546875, 0.9627685546875, -0.23001...  1  \n",
       "4379464  [0.16021728515625, 0.9654693603515625, -0.6411...  1  \n",
       "28297    [-0.09765625, 0.1728515625, 0.25, 0.3515625, -...  1  \n",
       "4844336  [3.5324554443359375, 0.277191162109375, 2.2111...  1  \n",
       "...                                                    ... ..  \n",
       "3962262  [0.4085693359375, 1.667144775390625, -0.440200...  0  \n",
       "1501108  [-0.83380126953125, 0.60302734375, -0.04724121...  0  \n",
       "2533959  [0.7264404296875, 0.86065673828125, -0.7277069...  0  \n",
       "48365    [-0.018688746861049106, 0.016660417829241072, ...  0  \n",
       "4145163  [-0.03807124385127315, 0.061185483579282406, 0...  0  \n",
       "\n",
       "[250000 rows x 5 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "googel_300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3aacd82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = googel_300['avg_vector']\n",
    "Y = googel_300['y']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state = 19, test_size = 0.2)\n",
    "\n",
    "x_train = np.array(x_train.tolist())\n",
    "\n",
    "y_train = np.array(y_train.tolist())\n",
    "\n",
    "x_train = np.nan_to_num(x_train)\n",
    "\n",
    "x_test = np.array(x_test.tolist())\n",
    "\n",
    "y_test = np.array(y_test.tolist())\n",
    "\n",
    "x_test = np.nan_to_num(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d9c33836",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of training set to use as validation\n",
    "#valid_size = 0.2\n",
    "\n",
    "DatasetTrain = TensorDataset(torch.from_numpy(x_train),torch.from_numpy(y_train))\n",
    "        \n",
    "DatasetTest=TensorDataset(torch.from_numpy(x_test),torch.from_numpy(y_test))\n",
    "\n",
    "train_loader=torch.utils.data.DataLoader(DatasetTrain,batch_size=batch_size,shuffle=True,drop_last=True, num_workers=0)\n",
    "\n",
    "valid_loader=torch.utils.data.DataLoader(DatasetTest, batch_size=batch_size, drop_last=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "89ad2a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # number of hidden nodes in each layer \n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        # linear layer (300 -> hidden_1)\n",
    "        self.fc1 = nn.Linear(300, hidden_1)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        # linear layer (n_hidden -> 10)\n",
    "        self.fc3 = nn.Linear(hidden_2, 3)\n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 300)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add output layer\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model = Net()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f3ec5753",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# specify loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f39b9b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.888198\n",
      "Epoch: 2 \tTraining Loss: 0.819735\n",
      "Epoch: 3 \tTraining Loss: 0.803842\n",
      "Epoch: 4 \tTraining Loss: 0.794879\n",
      "Epoch: 5 \tTraining Loss: 0.789147\n",
      "Epoch: 6 \tTraining Loss: 0.784515\n",
      "Epoch: 7 \tTraining Loss: 0.780191\n",
      "Epoch: 8 \tTraining Loss: 0.778002\n",
      "Epoch: 9 \tTraining Loss: 0.774452\n",
      "Epoch: 10 \tTraining Loss: 0.772460\n",
      "Epoch: 11 \tTraining Loss: 0.769765\n",
      "Epoch: 12 \tTraining Loss: 0.767997\n",
      "Epoch: 13 \tTraining Loss: 0.765558\n",
      "Epoch: 14 \tTraining Loss: 0.765464\n",
      "Epoch: 15 \tTraining Loss: 0.762639\n",
      "Epoch: 16 \tTraining Loss: 0.760900\n",
      "Epoch: 17 \tTraining Loss: 0.759239\n",
      "Epoch: 18 \tTraining Loss: 0.757018\n",
      "Epoch: 19 \tTraining Loss: 0.755700\n",
      "Epoch: 20 \tTraining Loss: 0.754198\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 20  # suggest training between 20-50 epochs\n",
    "\n",
    "model.train() # prep model for training\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data.float())\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ac92b109",
   "metadata": {},
   "outputs": [],
   "source": [
    "DatasetTest=TensorDataset(torch.from_numpy(x_test),torch.from_numpy(y_test))\n",
    "\n",
    "total_num = 0\n",
    "correct = 0\n",
    "for x_temp,y_real in DatasetTest:\n",
    "    res = model(x_temp.float())\n",
    "    _, pred = torch.max(res,1)\n",
    "    total_num += 1\n",
    "    if int(pred) == int(y_real):\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7d7e49b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_googel_avg_ternary =  correct/total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4a01c1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for ternary label from googel 300 model using average vector is  0.66946\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy for ternary label from googel 300 model using average vector is ', accuracy_googel_avg_ternary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882f08b6",
   "metadata": {},
   "source": [
    "# google news 300 for ternary label \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb7bdca",
   "metadata": {},
   "source": [
    "# concat vecter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0aa1dee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = googel_300['concat_vec']\n",
    "Y = googel_300['y']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state = 19, test_size = 0.2)\n",
    "\n",
    "x_train = np.array(x_train.tolist())\n",
    "\n",
    "y_train = np.array(y_train.tolist())\n",
    "\n",
    "x_train = np.nan_to_num(x_train)\n",
    "\n",
    "x_test = np.array(x_test.tolist())\n",
    "\n",
    "y_test = np.array(y_test.tolist())\n",
    "\n",
    "x_test = np.nan_to_num(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4a54eaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of training set to use as validation\n",
    "#valid_size = 0.2\n",
    "\n",
    "DatasetTrain = TensorDataset(torch.from_numpy(x_train),torch.from_numpy(y_train))\n",
    "        \n",
    "DatasetTest=TensorDataset(torch.from_numpy(x_test),torch.from_numpy(y_test))\n",
    "\n",
    "train_loader=torch.utils.data.DataLoader(DatasetTrain,batch_size=batch_size,shuffle=True,drop_last=True, num_workers=0)\n",
    "\n",
    "valid_loader=torch.utils.data.DataLoader(DatasetTest, batch_size=batch_size, drop_last=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b496210e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define the NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # number of hidden nodes in each layer \n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        # linear layer (300 -> hidden_1)\n",
    "        self.fc1 = nn.Linear(3000, hidden_1)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        # linear layer (n_hidden -> 10)\n",
    "        self.fc3 = nn.Linear(hidden_2, 3)\n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 3000)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add output layer\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model = Net()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "444a58c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# specify loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "33c1242b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.949798\n",
      "Epoch: 2 \tTraining Loss: 0.872999\n",
      "Epoch: 3 \tTraining Loss: 0.850158\n",
      "Epoch: 4 \tTraining Loss: 0.833774\n",
      "Epoch: 5 \tTraining Loss: 0.818439\n",
      "Epoch: 6 \tTraining Loss: 0.805019\n",
      "Epoch: 7 \tTraining Loss: 0.789864\n",
      "Epoch: 8 \tTraining Loss: 0.773915\n",
      "Epoch: 9 \tTraining Loss: 0.758746\n",
      "Epoch: 10 \tTraining Loss: 0.745903\n",
      "Epoch: 11 \tTraining Loss: 0.731850\n",
      "Epoch: 12 \tTraining Loss: 0.718900\n",
      "Epoch: 13 \tTraining Loss: 0.705711\n",
      "Epoch: 14 \tTraining Loss: 0.692805\n",
      "Epoch: 15 \tTraining Loss: 0.682410\n",
      "Epoch: 16 \tTraining Loss: 0.672709\n",
      "Epoch: 17 \tTraining Loss: 0.661839\n",
      "Epoch: 18 \tTraining Loss: 0.652909\n",
      "Epoch: 19 \tTraining Loss: 0.642177\n",
      "Epoch: 20 \tTraining Loss: 0.636413\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 20  # suggest training between 20-50 epochs\n",
    "\n",
    "model.train() # prep model for training\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data.float())\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c4a314b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_num = 0\n",
    "correct = 0\n",
    "for x_temp,y_real in DatasetTest:\n",
    "    res = model(x_temp.float())\n",
    "    _, pred = torch.max(res,1)\n",
    "    total_num += 1\n",
    "    if int(pred) == int(y_real):\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e9b8d3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_google_concat_ternary =  correct/total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3bbb7c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for ternary label from google 300 model using concat vector is  0.59616\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy for ternary label from google 300 model using concat vector is ', accuracy_google_concat_ternary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e3af2f",
   "metadata": {},
   "source": [
    "# google news 300 for binary label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d62fd0",
   "metadata": {},
   "source": [
    "# average vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d01d69f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "googel_300_binary = googel_300.loc[googel_300['label']!=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "660c6147",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = googel_300_binary['avg_vector']\n",
    "Y = googel_300_binary['y']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state = 19, test_size = 0.2)\n",
    "\n",
    "x_train = np.array(x_train.tolist())\n",
    "\n",
    "y_train = np.array(y_train.tolist())\n",
    "\n",
    "x_train = np.nan_to_num(x_train)\n",
    "\n",
    "x_test = np.array(x_test.tolist())\n",
    "\n",
    "y_test = np.array(y_test.tolist())\n",
    "\n",
    "x_test = np.nan_to_num(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "347e678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of training set to use as validation\n",
    "#valid_size = 0.2\n",
    "\n",
    "DatasetTrain = TensorDataset(torch.from_numpy(x_train),torch.from_numpy(y_train))\n",
    "        \n",
    "DatasetTest=TensorDataset(torch.from_numpy(x_test),torch.from_numpy(y_test))\n",
    "\n",
    "train_loader=torch.utils.data.DataLoader(DatasetTrain,batch_size=batch_size,shuffle=True,drop_last=True, num_workers=0)\n",
    "\n",
    "valid_loader=torch.utils.data.DataLoader(DatasetTest, batch_size=batch_size, drop_last=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e130f51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define the NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # number of hidden nodes in each layer \n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        # linear layer (300 -> hidden_1)\n",
    "        self.fc1 = nn.Linear(300, hidden_1)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        # linear layer (n_hidden -> 10)\n",
    "        self.fc3 = nn.Linear(hidden_2, 2)\n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 300)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add output layer\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model = Net()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "724c24b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# specify loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "32beceec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.516591\n",
      "Epoch: 2 \tTraining Loss: 0.436113\n",
      "Epoch: 3 \tTraining Loss: 0.418995\n",
      "Epoch: 4 \tTraining Loss: 0.408833\n",
      "Epoch: 5 \tTraining Loss: 0.401627\n",
      "Epoch: 6 \tTraining Loss: 0.398218\n",
      "Epoch: 7 \tTraining Loss: 0.394187\n",
      "Epoch: 8 \tTraining Loss: 0.390971\n",
      "Epoch: 9 \tTraining Loss: 0.386998\n",
      "Epoch: 10 \tTraining Loss: 0.383374\n",
      "Epoch: 11 \tTraining Loss: 0.381110\n",
      "Epoch: 12 \tTraining Loss: 0.378476\n",
      "Epoch: 13 \tTraining Loss: 0.376614\n",
      "Epoch: 14 \tTraining Loss: 0.374721\n",
      "Epoch: 15 \tTraining Loss: 0.373161\n",
      "Epoch: 16 \tTraining Loss: 0.370029\n",
      "Epoch: 17 \tTraining Loss: 0.369939\n",
      "Epoch: 18 \tTraining Loss: 0.366963\n",
      "Epoch: 19 \tTraining Loss: 0.366842\n",
      "Epoch: 20 \tTraining Loss: 0.364092\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 20  # suggest training between 20-50 epochs\n",
    "\n",
    "model.train() # prep model for training\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data.float())\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "bbd94438",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_num = 0\n",
    "correct = 0\n",
    "for x_temp,y_real in DatasetTest:\n",
    "    res = model(x_temp.float())\n",
    "    _, pred = torch.max(res,1)\n",
    "    total_num += 1\n",
    "    if int(pred) == int(y_real):\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ab0e14f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_googel_avg_binary =  correct/total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "79735a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for binary label from google 300 model using avg vector is  0.831475\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy for binary label from google 300 model using avg vector is ', accuracy_googel_avg_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d186c08",
   "metadata": {},
   "source": [
    "# google news 300 for binary label\n",
    "concat vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9024374",
   "metadata": {},
   "source": [
    "# concat vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "94e4dff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = googel_300_binary['concat_vec']\n",
    "Y = googel_300_binary['y']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state = 19, test_size = 0.2)\n",
    "\n",
    "x_train = np.array(x_train.tolist())\n",
    "\n",
    "y_train = np.array(y_train.tolist())\n",
    "\n",
    "x_train = np.nan_to_num(x_train)\n",
    "\n",
    "x_test = np.array(x_test.tolist())\n",
    "\n",
    "y_test = np.array(y_test.tolist())\n",
    "\n",
    "x_test = np.nan_to_num(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "baae5657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of training set to use as validation\n",
    "#valid_size = 0.2\n",
    "\n",
    "DatasetTrain = TensorDataset(torch.from_numpy(x_train),torch.from_numpy(y_train))\n",
    "        \n",
    "DatasetTest=TensorDataset(torch.from_numpy(x_test),torch.from_numpy(y_test))\n",
    "\n",
    "train_loader=torch.utils.data.DataLoader(DatasetTrain,batch_size=batch_size,shuffle=True,drop_last=True, num_workers=0)\n",
    "\n",
    "valid_loader=torch.utils.data.DataLoader(DatasetTest, batch_size=batch_size, drop_last=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d47fc308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define the NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # number of hidden nodes in each layer \n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        # linear layer (300 -> hidden_1)\n",
    "        self.fc1 = nn.Linear(3000, hidden_1)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        # linear layer (n_hidden -> 10)\n",
    "        self.fc3 = nn.Linear(hidden_2, 2)\n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 3000)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add output layer\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model = Net()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "75ae107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# specify loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "58e3b6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.555557\n",
      "Epoch: 2 \tTraining Loss: 0.485170\n",
      "Epoch: 3 \tTraining Loss: 0.467148\n",
      "Epoch: 4 \tTraining Loss: 0.450466\n",
      "Epoch: 5 \tTraining Loss: 0.433529\n",
      "Epoch: 6 \tTraining Loss: 0.415610\n",
      "Epoch: 7 \tTraining Loss: 0.398995\n",
      "Epoch: 8 \tTraining Loss: 0.381185\n",
      "Epoch: 9 \tTraining Loss: 0.362999\n",
      "Epoch: 10 \tTraining Loss: 0.346290\n",
      "Epoch: 11 \tTraining Loss: 0.331080\n",
      "Epoch: 12 \tTraining Loss: 0.314752\n",
      "Epoch: 13 \tTraining Loss: 0.301338\n",
      "Epoch: 14 \tTraining Loss: 0.288892\n",
      "Epoch: 15 \tTraining Loss: 0.276443\n",
      "Epoch: 16 \tTraining Loss: 0.264201\n",
      "Epoch: 17 \tTraining Loss: 0.254010\n",
      "Epoch: 18 \tTraining Loss: 0.243546\n",
      "Epoch: 19 \tTraining Loss: 0.236213\n",
      "Epoch: 20 \tTraining Loss: 0.229487\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 20  # suggest training between 20-50 epochs\n",
    "\n",
    "model.train() # prep model for training\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data.float())\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "348767a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num = 0\n",
    "correct = 0\n",
    "for x_temp,y_real in DatasetTest:\n",
    "    res = model(x_temp.float())\n",
    "    _, pred = torch.max(res,1)\n",
    "    total_num += 1\n",
    "    if int(pred) == int(y_real):\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5a7e27e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_googel_concat_binary =  correct/total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "21909ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for binary label from google 300 model using concat vector is  0.831475\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy for binary label from google 300 model using concat vector is ', accuracy_googel_avg_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2a6eae",
   "metadata": {},
   "source": [
    "## conclusion\n",
    "The accuracy for binary label from google 300 model using avg vector is  0.831475\n",
    "\n",
    "The accuracy for binary label from google 300 model using concat vector is  0.831475\n",
    "\n",
    "The accuracy for ternary label from google 300 model using concat vector is  0.59616\n",
    "\n",
    "The accuracy for ternary label from googel 300 model using average vector is  0.66946\n",
    "\n",
    "The accuracy for ternary label from my model using concat vector is  0.61106\n",
    "\n",
    "The accuracy for ternary label from my model using average vector is  0.6855"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfe90e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
